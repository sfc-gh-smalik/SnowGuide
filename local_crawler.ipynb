{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "42tw5jv7lfo4rkfix3dn",
   "authorId": "368576329414",
   "authorName": "SMALIK",
   "authorEmail": "sunny.malik@snowflake.com",
   "sessionId": "0730821e-d34b-434e-9e71-da84f1bb6e6b",
   "lastEditTime": 1750191099852
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": true,
    "codeCollapsed": true
   },
   "source": "# # Import python packages\n# import streamlit as st\n# import pandas as pd\n\n# # We can also use Snowpark for our analyses!\n# from snowflake.snowpark.context import get_active_session\n# session = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3d8ce9dc-5214-4e9c-9fc6-4be69bc8d47d",
   "metadata": {
    "name": "Prerequsities",
    "collapsed": false
   },
   "source": "Install the required libraries for the Crawler and ensure the external access Integration is configured and enabled for PyPi and for docs.snowflake.com and quickstarts.snowflake.com"
  },
  {
   "cell_type": "code",
   "id": "1cde9a43-20f0-4148-94f6-640e8c3b5bfe",
   "metadata": {
    "language": "python",
    "name": "pip_install",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install beautifulsoup4 langdetect dotenv",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "crawler",
    "collapsed": true,
    "codeCollapsed": true
   },
   "source": "import requests\nfrom bs4 import BeautifulSoup, Comment, SoupStrainer\nimport time\nimport logging\nimport hashlib\nimport sys\nfrom urllib.parse import urljoin, urlparse, urlunparse\nfrom collections import defaultdict, deque\nfrom typing import Set, Dict, List, Optional, Tuple, Union\nimport re\nfrom datetime import datetime, timedelta\nimport json\nimport os\nimport csv\nfrom dataclasses import dataclass, asdict\nfrom langdetect import detect\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom queue import Queue, Empty\nimport random\nimport psutil\nimport weakref\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.types import *\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@dataclass\nclass CrawlResult:\n    url: str\n    found_on: str\n    depth: int\n    timestamp: str\n    content_hash: str\n    page_title: str\n    status_code: int\n    language: str\n    last_visited: str = \"\"\n    content_changed: bool = False\n    previous_hash: str = \"\"\n    visit_count: int = 1\n    # Enhanced content fields for database storage\n    raw_html: str = \"\"\n    cleaned_text: str = \"\"\n    content_size: int = 0\n    content_type: str = \"\"\n    extracted_links: List[str] = None\n    meta_description: str = \"\"\n    meta_keywords: str = \"\"\n    headings: Dict[str, List[str]] = None\n    images: List[Dict[str, str]] = None\n    structured_data: Dict = None\n\n    def __post_init__(self):\n        if self.extracted_links is None:\n            self.extracted_links = []\n        if self.headings is None:\n            self.headings = {}\n        if self.images is None:\n            self.images = []\n        if self.structured_data is None:\n            self.structured_data = {}\n\n\nclass MemoryMonitor:\n    \"\"\"Monitor memory usage and implement cache size limits.\"\"\"\n\n    def __init__(self, max_memory_mb: int = 1024):\n        self.max_memory_mb = max_memory_mb\n        self.process = psutil.Process()\n\n    def get_memory_usage_mb(self) -> float:\n        \"\"\"Get current memory usage in MB.\"\"\"\n        return self.process.memory_info().rss / 1024 / 1024\n\n    def is_memory_limit_exceeded(self) -> bool:\n        \"\"\"Check if memory limit is exceeded.\"\"\"\n        return self.get_memory_usage_mb() > self.max_memory_mb\n\n    def log_memory_usage(self):\n        \"\"\"Log current memory usage.\"\"\"\n        usage = self.get_memory_usage_mb()\n        logging.info(f\"Memory usage: {usage:.2f} MB\")\n\n\nclass URLValidator:\n    \"\"\"Enhanced URL validation with security checks.\"\"\"\n\n    @staticmethod\n    def is_valid_https_url(url: str) -> bool:\n        \"\"\"Validate HTTPS URL with security checks.\"\"\"\n        if not url or not isinstance(url, str):\n            return False\n\n        try:\n            parsed = urlparse(url)\n\n            # Must be HTTPS\n            if parsed.scheme != 'https':\n                return False\n\n            # Must have valid netloc\n            if not parsed.netloc:\n                return False\n\n            # Block localhost and private IPs\n            if any(blocked in parsed.netloc.lower() for blocked in\n                   ['localhost', '127.0.0.1', '0.0.0.0', '10.', '192.168.', '172.']):\n                return False\n\n            # Block suspicious file extensions\n            suspicious_extensions = ['.exe', '.zip', '.pdf', '.doc', '.docx', '.xls', '.xlsx']\n            if any(parsed.path.lower().endswith(ext) for ext in suspicious_extensions):\n                return False\n\n            return True\n        except Exception:\n            return False\n\n\ndef detect_language_from_url_path(url: str) -> Optional[str]:\n    \"\"\"Detect language from URL path patterns like /en/, /fr/, /de/, etc.\"\"\"\n    if not url:\n        return None\n\n    try:\n        parsed_url = urlparse(url)\n        path = parsed_url.path.lower()\n\n        language_patterns = {\n            'en': [r'/en/'],\n            'fr': [r'/fr/'],\n            'de': [r'/de/'],\n            'ja': [r'/ja/'],\n            'ko': [r'/ko/'],\n            'pt': [r'/pt/'],\n            'es': [r'/es/'],\n            'it': [r'/it/'],\n            'ru': [r'/ru/'],\n            'zh': [r'/zh/']\n        }\n\n        for lang, patterns in language_patterns.items():\n            for pattern in patterns:\n                if re.search(pattern, path):\n                    return lang\n\n        if parsed_url.netloc:\n            subdomain = parsed_url.netloc.split('.')[0].lower()\n            for lang in language_patterns.keys():\n                if subdomain == lang or subdomain.startswith(f\"{lang}-\"):\n                    return lang\n\n        if parsed_url.query:\n            query_lower = parsed_url.query.lower()\n            for lang in language_patterns.keys():\n                if f\"lang={lang}\" in query_lower or f\"language={lang}\" in query_lower:\n                    return lang\n\n        return None\n\n    except Exception:\n        return None\n\n\nclass GracefulKiller:\n    \"\"\"Handle graceful shutdown without using signals.\"\"\"\n\n    def __init__(self, timeout_hours: Optional[float] = None):\n        self.kill_now = threading.Event()\n        self.start_time = time.time()\n        self.timeout_hours = timeout_hours\n\n    def stop(self):\n        \"\"\"Stop the crawler gracefully.\"\"\"\n        self.kill_now.set()\n\n    # def should_stop(self) -> bool:\n    #     \"\"\"Check if we should stop.\"\"\"\n    #     return self.kill_now.is_set()\n\n    def should_stop(self) -> bool:\n        \"\"\"Check if we should stop due to manual intervention or timeout.\"\"\"\n        if self.kill_now.is_set():\n            return True\n\n        if self.timeout_hours:\n            elapsed_hours = (time.time() - self.start_time) / 3600\n            if elapsed_hours >= self.timeout_hours:\n                logging.info(f\"Maximum runtime of {self.timeout_hours} hours reached. Stopping crawler.\")\n                self.stop()\n                return True\n\n        return False\n\n\nclass DatabaseConnectionPool:\n    \"\"\"Connection pool for database operations.\"\"\"\n\n    def __init__(self, config: Dict, pool_size: int = 5):\n        self.config = config\n        self.pool_size = pool_size\n        self.connections = Queue(maxsize=pool_size)\n        self.lock = threading.Lock()\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool.\"\"\"\n        try:\n            import snowflake.connector\n            for _ in range(self.pool_size):\n                conn = snowflake.connector.connect(**self.config)\n                self.connections.put(conn)\n        except Exception as e:\n            logging.error(f\"Failed to initialize connection pool: {e}\")\n\n    def get_connection(self):\n        \"\"\"Get connection from pool.\"\"\"\n        try:\n            return self.connections.get(timeout=5)\n        except Empty:\n            # Create new connection if pool is empty\n            import snowflake.connector\n            return snowflake.connector.connect(**self.config)\n\n    def return_connection(self, conn):\n        \"\"\"Return connection to pool.\"\"\"\n        try:\n            if conn and not conn.is_closed():\n                self.connections.put(conn, timeout=1)\n        except:\n            # Connection is bad, don't return to pool\n            pass\n\n\ndef test_snowflake_connection():\n    \"\"\"Test Snowflake connection before crawling.\"\"\"\n    try:\n        from snowflake.snowpark.context import get_active_session\n        session = get_active_session()\n        result = session.sql(\"SELECT CURRENT_VERSION()\").collect()\n        print(f\"Snowflake connection successful: {result[0][0]}\")\n        return True\n    except Exception as e:\n        print(f\"Snowflake connection failed: {e}\")\n        return False\n\n\nclass CSVStorage:\n    \"\"\"CSV-based storage implementation with similar functionality to SnowparkStorage.\"\"\"\n\n    def __init__(self, base_filename: str = \"crawler\", max_cache_size: int = 10000):\n        self.base_filename = base_filename\n        self.max_cache_size = max_cache_size\n        self.lock = threading.RLock()\n        self.memory_monitor = MemoryMonitor()\n\n        self.table_prefix = base_filename\n\n        # File paths\n        self.discovered_urls_file = f\"{base_filename}_discovered_urls.csv\"\n        self.visited_urls_file = f\"{base_filename}_visited_urls.csv\"\n        self.content_hashes_file = f\"{base_filename}_content_hashes.csv\"\n        self.crawler_state_file = f\"{base_filename}_state.json\"\n        self.crawler_revisit_file = f\"{self.base_filename}_revisit_tracking.csv\"\n\n        # In-memory cache\n        self.visited_cache = set()\n        self.content_hash_cache = set()\n        self.url_cache = set()\n\n        # Batch processing\n        self.batch_size = 10\n        self.visited_urls_batch = []\n        self.content_hashes_batch = []\n        self.discovered_urls_batch = []\n\n        # Check if files exist, if not try to generate from Snowflake\n        files_exist = all(os.path.exists(f) for f in [\n            self.discovered_urls_file,\n            self.visited_urls_file,\n            self.content_hashes_file\n        ])\n\n        if not files_exist:\n            logging.info(\"Local CSV files not found. Attempting to generate from Snowflake...\")\n            self._generate_files_from_snowflake()\n\n        # Create files if they don't exist\n        self._create_files()\n        self._load_caches()\n        \n\n    def _create_files(self):\n        \"\"\"Create CSV files with headers and validate column structure.\"\"\"\n        with self.lock:\n            # Define expected columns for each file\n            discovered_urls_columns = [\n                'url', 'found_on', 'depth', 'timestamp', 'content_hash',\n                'page_title', 'status_code', 'language', 'last_visited',\n                'content_changed', 'previous_hash', 'visit_count',\n                'next_revisit_time', 'created_at', 'updated_at',\n                'cleaned_text', 'content_size', 'content_type',\n                'extracted_links', 'meta_description', 'meta_keywords',\n                'headings', 'images', 'structured_data'\n            ]\n\n            visited_urls_columns = ['url', 'visited_at']\n            content_hashes_columns = ['content_hash', 'first_url', 'created_at']\n            \n            # Helper function to validate and create file\n            def validate_and_create_file(filepath, expected_columns):\n                needs_creation = True\n                if os.path.exists(filepath):\n                    try:\n                        with open(filepath, 'r', newline='', encoding='utf-8') as f:\n                            reader = csv.reader(f)\n                            existing_columns = next(reader, [])\n                            if existing_columns == expected_columns:\n                                needs_creation = False\n                            else:\n                                print(f\"Invalid column structure in {filepath}. Recreating file.\")\n                    except Exception as e:\n                        print(f\"Error reading {filepath}: {e}. Recreating file.\")\n                \n                if needs_creation:\n                    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n                        writer = csv.writer(f)\n                        writer.writerow(expected_columns)\n                    print(f\"Created {filepath} with {len(expected_columns)} columns\")\n            \n            # Create and validate each file\n            validate_and_create_file(self.discovered_urls_file, discovered_urls_columns)\n            validate_and_create_file(self.visited_urls_file, visited_urls_columns)\n            validate_and_create_file(self.content_hashes_file, content_hashes_columns)\n            \n            # Validate file creation\n            files_to_check = [\n                (self.discovered_urls_file, discovered_urls_columns),\n                (self.visited_urls_file, visited_urls_columns),\n                (self.content_hashes_file, content_hashes_columns)\n            ]\n            \n            validation_errors = []\n            for filepath, expected_columns in files_to_check:\n                if not os.path.exists(filepath):\n                    validation_errors.append(f\"Failed to create {filepath}\")\n                else:\n                    try:\n                        with open(filepath, 'r', newline='', encoding='utf-8') as f:\n                            reader = csv.reader(f)\n                            actual_columns = next(reader, [])\n                            if actual_columns != expected_columns:\n                                validation_errors.append(\n                                    f\"Column mismatch in {filepath}. \"\n                                    f\"Expected {len(expected_columns)} columns, got {len(actual_columns)}\"\n                                )\n                    except Exception as e:\n                        validation_errors.append(f\"Error validating {filepath}: {e}\")\n            \n            if validation_errors:\n                error_msg = \"\\n\".join(validation_errors)\n                raise RuntimeError(f\"File validation failed:\\n{error_msg}\")\n\n\n    def _load_caches(self):\n        \"\"\"Load existing data into memory caches.\"\"\"\n        from datetime import datetime\n        try:\n            # Load visited URLs\n            if os.path.exists(self.visited_urls_file):\n                with open(self.visited_urls_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    self.visited_cache = {row['url'] for row in reader}\n\n            # Load content hashes\n            if os.path.exists(self.content_hashes_file):\n                with open(self.content_hashes_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    self.content_hash_cache = {row['content_hash'] for row in reader}\n\n            # Load discovered URLs\n            if os.path.exists(self.discovered_urls_file):\n                with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    self.url_cache = {row['url'] for row in reader}\n\n            # Load revisit tracking data\n            if os.path.exists(self.crawler_revisit_file):\n                with open(self.crawler_revisit_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    self.revisit_tracking = {\n                        row['url']: {\n                            'last_hash': row['last_hash'],\n                            'last_revisit': row['last_revisit'],\n                            'next_revisit': row['next_revisit'],\n                            'revisit_interval_hours': int(row['revisit_interval_hours']),\n                            'consecutive_unchanged': int(row['consecutive_unchanged'])\n                        } for row in reader\n                    }\n            else:\n                self.revisit_tracking = {}\n\n\n            # Load state file\n            if os.path.exists(self.crawler_state_file):\n                with open(self.crawler_state_file, 'r', encoding='utf-8') as f:\n                    try:\n                        self.state_data = json.load(f)\n                    except json.JSONDecodeError:\n                        self.state_data = {\n                            'pending_urls': [],\n                            'pages_processed': 0,\n                            'new_urls_found': 0,\n                            'error_categories': {},\n                            'timestamp': datetime.now().isoformat()\n                        }\n            else:\n                self.state_data = {\n                    'pending_urls': [],\n                    'pages_processed': 0,\n                    'new_urls_found': 0,\n                    'error_categories': {},\n                    'timestamp': datetime.now().isoformat()\n                }\n\n\n        except Exception as e:\n            logging.error(f\"Error loading caches: {e}\")\n\n    def _generate_files_from_snowflake(self):\n        \"\"\"Generate local CSV files from existing Snowflake tables.\"\"\"\n        try:\n            from snowflake.snowpark.context import get_active_session\n            session = get_active_session()\n\n            # Check if Snowflake tables exist and create corresponding CSV files\n            tables_to_check = [\n                (f\"{self.table_prefix.upper()}_DISCOVERED_URLS\", self.discovered_urls_file),\n                (f\"{self.table_prefix.upper()}_VISITED_URLS\", self.visited_urls_file),\n                (f\"{self.table_prefix.upper()}_CONTENT_HASHES\", self.content_hashes_file),\n                (f\"{self.table_prefix.upper()}_REVISIT_SCHEDULE\", self.crawler_revisit_file),  # Added revisit schedule table\n                (f\"{self.table_prefix.upper()}_STATE\", self.crawler_state_file)  # Added state table\n            ]\n\n            for table_name, csv_file in tables_to_check:\n                try:\n                    # Check if table exists\n                    check_table = f\"SHOW TABLES LIKE '{table_name}'\"\n                    result = session.sql(check_table).collect()\n\n                    if result:\n                        # Export table data to CSV\n                        query = f\"SELECT * FROM {table_name}\"\n                        df = session.sql(query).to_pandas()\n\n                        # Convert column names to lowercase\n                        df.columns = df.columns.str.lower()\n\n                        # Special handling for state table - convert to JSON\n                        if table_name.endswith('_STATE'):\n                            if not df.empty:\n                                # Get the latest state record\n                                latest_state = df.iloc[-1].to_dict()\n                                import json\n                                with open(csv_file, 'w') as f:\n                                    json.dump(latest_state, f)\n\n                        else:\n                            # Write to CSV file\n                            df.to_csv(csv_file, index=False)\n                            \n                        # logging.info(f\"Generated {csv_file} from {table_name}\")\n                        print(f\"Generated {csv_file} from {table_name}\")\n\n                except Exception as e:\n                    print(f\"Could not generate {csv_file} from {table_name}: {e}\")\n                    continue\n\n        except Exception as e:\n            print(f\"Could not connect to Snowflake to generate local files: {e}\")\n\n    def _flush_batches(self):\n        \"\"\"Flush all pending batches to CSV files with complete column set.\"\"\"\n        import datetime\n\n        with self.lock:\n            try:\n                # Flush visited URLs\n                if self.visited_urls_batch:\n                    with open(self.visited_urls_file, 'a', newline='', encoding='utf-8') as f:\n                        writer = csv.writer(f)\n                        current_time = datetime.datetime.now().isoformat()\n                        for url in self.visited_urls_batch:\n                            writer.writerow([url, current_time])\n                    self.visited_urls_batch.clear()\n\n                # Flush content hashes\n                if self.content_hashes_batch:\n                    with open(self.content_hashes_file, 'a', newline='', encoding='utf-8') as f:\n                        writer = csv.writer(f)\n                        current_time = datetime.datetime.now().isoformat()\n                        for content_hash, url in self.content_hashes_batch:\n                            writer.writerow([content_hash, url, current_time])\n                    self.content_hashes_batch.clear()\n\n                # Flush discovered URLs with all columns matching Snowflake schema\n                if self.discovered_urls_batch:\n                    with open(self.discovered_urls_file, 'a', newline='', encoding='utf-8') as f:\n                        writer = csv.writer(f)\n                        current_time = datetime.datetime.now().isoformat()\n                        next_revisit_time = (datetime.datetime.now() + datetime.timedelta(hours=24)).isoformat()\n\n                        for result in self.discovered_urls_batch:\n                            # Create a row with all columns matching Snowflake schema\n                            writer.writerow([\n                                result.url,  # URL\n                                result.found_on,  # FOUND_ON\n                                result.depth,  # DEPTH\n                                result.timestamp,  # TIMESTAMP\n                                result.content_hash,  # CONTENT_HASH\n                                result.page_title,  # PAGE_TITLE\n                                result.status_code,  # STATUS_CODE\n                                result.language,  # LANGUAGE\n                                current_time,  # LAST_VISITED\n                                result.content_changed,  # CONTENT_CHANGED\n                                result.previous_hash,  # PREVIOUS_HASH\n                                result.visit_count,  # VISIT_COUNT\n                                next_revisit_time,  # NEXT_REVISIT_TIME\n                                current_time,  # CREATED_AT\n                                current_time,  # UPDATED_AT\n                                # '',  # RAW_HTML\n                                result.cleaned_text,  # CLEANED_TEXT\n                                result.content_size or 0,  # CONTENT_SIZE\n                                result.content_type,  # CONTENT_TYPE\n                                json.dumps(result.extracted_links),  # EXTRACTED_LINKS\n                                result.meta_description,  # META_DESCRIPTION\n                                result.meta_keywords,  # META_KEYWORDS\n                                json.dumps(result.headings),  # HEADINGS\n                                json.dumps(result.images),  # IMAGES\n                                json.dumps(result.structured_data)  # STRUCTURED_DATA\n                            ])\n                    self.discovered_urls_batch.clear()\n\n            except Exception as e:\n                print(f\"Error flushing batches: {e}\")\n\n    def check_content_change(self, url: str, new_content_hash: str) -> Tuple[bool, str]:\n        \"\"\"Check if content has changed for CSV storage mode.\"\"\"\n        with self.lock:\n            try:\n                if os.path.exists(self.discovered_urls_file):\n                    with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:\n                        reader = csv.DictReader(f)\n\n                        for row in reader:\n                            if row['url'].strip() == url:\n                                previous_hash = row['content_hash'].strip()\n                                content_changed = new_content_hash != previous_hash\n                                return content_changed, previous_hash\n\n                    # URL not found, treat as new content\n                    return True, \"\"\n\n                # File doesn't exist, treat as new content\n                return True, \"\"\n\n            except Exception as e:\n                print(f\"*****Error checking content change: {e}\")\n                return True, \"\"\n\n    def _update_revisit_tracking(self, url: str, new_hash: str, content_changed: bool):\n        \"\"\"Update revisit tracking information in CSV.\"\"\"\n        import datetime\n\n        revisit_file = self.crawler_revisit_file\n\n        # Create file if it doesn't exist\n        if not os.path.exists(revisit_file):\n            with open(revisit_file, 'w', newline='', encoding='utf-8') as f:\n                writer = csv.writer(f)\n                writer.writerow(['url', 'last_hash', 'last_revisit', 'next_revisit',\n                                 'revisit_interval_hours', 'consecutive_unchanged'])\n\n        current_time = datetime.datetime.now()\n        rows = []\n        found = False\n        # print('***************Sunny written data to revisit tracking*************')\n        # Read existing data\n        with open(revisit_file, 'r', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                if row['url'] == url:\n                    found = True\n                    # Update existing entry\n                    if content_changed:\n                        # Reset interval on change\n                        next_revisit = current_time + timedelta(hours=24)\n                        row.update({\n                            'last_hash': new_hash,\n                            'last_revisit': current_time.isoformat(),\n                            'next_revisit': next_revisit.isoformat(),\n                            'revisit_interval_hours': '24',\n                            'consecutive_unchanged': '0'\n                        })\n                    else:\n                        # Increase interval if unchanged\n                        consecutive = int(row.get('consecutive_unchanged', 0)) + 1\n                        interval = min(int(row.get('revisit_interval_hours', 24)) * 2, 168)\n                        next_revisit = current_time + timedelta(hours=interval)\n                        row.update({\n                            'last_revisit': current_time.isoformat(),\n                            'next_revisit': next_revisit.isoformat(),\n                            'revisit_interval_hours': str(interval),\n                            'consecutive_unchanged': str(consecutive)\n                        })\n                rows.append(row)\n\n        # Add new entry if URL not found\n        if not found:\n            next_revisit = current_time + timedelta(hours=24)\n            rows.append({\n                'url': url,\n                'last_hash': new_hash,\n                'last_revisit': current_time.isoformat(),\n                'next_revisit': next_revisit.isoformat(),\n                'revisit_interval_hours': '24',\n                'consecutive_unchanged': '0'\n            })\n\n        # Write updated data\n        with open(revisit_file, 'w', newline='', encoding='utf-8') as f:\n            if rows:\n                writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n                writer.writeheader()\n                writer.writerows(rows)\n\n    def check_duplicate_content(self, url: str, content_hash: str) -> Tuple[bool, Optional[str]]:\n        \"\"\"Check if content is duplicate.\"\"\"\n        with self.lock:\n            try:\n                if os.path.exists(self.content_hashes_file):\n                    with open(self.content_hashes_file, 'r', encoding='utf-8') as f:\n                        reader = csv.DictReader(f)\n                        for row in reader:\n                            if row['content_hash'] == content_hash:\n                                if row['first_url'] != url:\n                                    return True, row['first_url']\n                return False, None\n            except Exception as e:\n                logging.error(f\"Error checking duplicate content: {e}\")\n                return False, None\n\n    def mark_url_visited(self, url: str):\n        \"\"\"Mark URL as visited with batching.\"\"\"\n        with self.lock:\n            if url not in self.visited_cache:\n                self.visited_urls_batch.append(url)\n                self.visited_cache.add(url)\n                if len(self.visited_urls_batch) >= self.batch_size:\n                    self._flush_batches()\n\n    def add_content_hash(self, content_hash: str, url: str):\n        \"\"\"Add content hash with batching.\"\"\"\n        with self.lock:\n            if content_hash not in self.content_hash_cache:\n                self.content_hashes_batch.append((content_hash, url))\n                self.content_hash_cache.add(content_hash)\n                if len(self.content_hashes_batch) >= self.batch_size:\n                    self._flush_batches()\n\n    def insert_discovered_url_with_content(self, crawl_result: CrawlResult) -> bool:\n        \"\"\"Insert discovered URL with full content data.\"\"\"\n        with self.lock:\n            if crawl_result.url not in self.url_cache:\n                self.discovered_urls_batch.append(crawl_result)\n                self.url_cache.add(crawl_result.url)\n                if len(self.discovered_urls_batch) >= self.batch_size:\n                    # print('Sunny Calling from insert_discovered_url_with_content -_flush_batches ')\n                    self._flush_batches()\n                return True\n            return False\n\n    def get_urls_for_revisit(self, max_age_hours: int = 24, limit: int = 100) -> List[Tuple[str, str, int]]:\n        \"\"\"Get URLs that are due for revisit based on their last visit time.\"\"\"\n        try:\n            from datetime import datetime\n            revisit_urls = []\n            current_time = datetime.now()\n\n            if os.path.exists(self.crawler_revisit_file):\n                with open(self.crawler_revisit_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        try:\n                            next_revisit = datetime.fromisoformat(row['next_revisit'])\n                            if next_revisit <= current_time:\n                                # Get the corresponding URL data from discovered_urls file\n                                with open(self.discovered_urls_file, 'r', encoding='utf-8') as df:\n                                    url_reader = csv.DictReader(df)\n                                    for url_row in url_reader:\n                                        if url_row['url'] == row['url']:\n                                            revisit_urls.append((\n                                                url_row['url'],\n                                                url_row['found_on'],\n                                                int(url_row['depth'])\n                                            ))\n                                            break\n                        except (ValueError, KeyError) as e:\n                            logging.error(f\"Error processing revisit entry: {e}\")\n                            continue\n\n                        if len(revisit_urls) >= limit:\n                            break\n\n            return revisit_urls[:limit]\n\n        except Exception as e:\n            logging.error(f\"Error getting URLs for revisit: {e}\")\n            return []\n\n    def get_unvisited_discovered_urls(self, limit: int = 100) -> List[Tuple[str, str, int]]:\n        \"\"\"Get unvisited URLs from the discovered URLs file.\"\"\"\n        try:\n            unvisited_urls = []\n            visited_urls = set()\n\n            # Load visited URLs into a set for faster lookup\n            if os.path.exists(self.visited_urls_file):\n                with open(self.visited_urls_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    visited_urls = {row['url'] for row in reader}\n\n            # Find unvisited URLs from discovered URLs\n            if os.path.exists(self.discovered_urls_file):\n                with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        url = row['url']\n                        if url not in visited_urls:\n                            try:\n                                unvisited_urls.append((\n                                    url,\n                                    row['found_on'],\n                                    int(row['depth']) + 1\n                                ))\n                                if len(unvisited_urls) >= limit:\n                                    break\n                            except (ValueError, KeyError) as e:\n                                logging.error(f\"Error processing discovered URL entry: {e}\")\n                                continue\n\n            return unvisited_urls[:limit]\n\n        except Exception as e:\n            logging.error(f\"Error getting unvisited URLs: {e}\")\n            return []\n\n    def url_exists(self, url: str) -> bool:\n        \"\"\"Check if URL exists.\"\"\"\n        return url in self.url_cache\n\n    def is_url_visited(self, url: str) -> bool:\n        \"\"\"Check if URL has been visited.\"\"\"\n        return url in self.visited_cache\n\n    def get_discovered_urls_count(self) -> int:\n        \"\"\"Get count of discovered URLs.\"\"\"\n        return len(self.url_cache)\n\n    def get_visited_urls_count(self) -> int:\n        \"\"\"Get count of visited URLs.\"\"\"\n        return len(self.visited_cache)\n\n    def save_crawler_state(self, state_data: Dict):\n        \"\"\"Save crawler state to JSON file.\"\"\"\n        with self.lock:\n            try:\n                with open(self.crawler_state_file, 'w', encoding='utf-8') as f:\n                    json.dump(state_data, f)\n            except Exception as e:\n                logging.error(f\"Error saving crawler state: {e}\")\n\n    def load_crawler_state(self) -> Dict:\n        \"\"\"Load crawler state from JSON file.\"\"\"\n        with self.lock:\n            try:\n                if os.path.exists(self.crawler_state_file):\n                    with open(self.crawler_state_file, 'r', encoding='utf-8') as f:\n                        return json.load(f)\n                return {}\n            except Exception as e:\n                logging.error(f\"Error loading crawler state: {e}\")\n                return {}\n\n    def get_statistics(self) -> Dict:\n        \"\"\"Get crawling statistics.\"\"\"\n        stats = {\n            'total_discovered': len(self.url_cache),\n            'total_visited': len(self.visited_cache),\n            'language_distribution': defaultdict(int),\n            'depth_distribution': defaultdict(int),\n            'status_code_distribution': defaultdict(int)\n        }\n\n        try:\n            with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    stats['language_distribution'][row['language']] += 1\n                    stats['depth_distribution'][int(row['depth'])] += 1\n                    stats['status_code_distribution'][int(row['status_code'])] += 1\n        except Exception as e:\n            logging.error(f\"Error getting statistics: {e}\")\n\n        return stats\n\n    def close(self):\n        \"\"\"Close and flush any pending operations.\"\"\"\n        self._flush_batches()\n\n\ndef upload_csv_to_snowflake(snowflake_config: dict, table_prefix: str) -> bool:\n    \"\"\"\n    Upload CSV files to Snowflake tables using Snowpark DataFrame with merge operations.\n    \"\"\"\n    try:\n        from snowflake.snowpark import Session\n        # from snowflake.snowpark.types import StructType, StructField, StringType, IntegerType, TimestampType, \\\n        #     BooleanType, VariantType\n        import pandas as pd\n        import os\n        from snowflake.snowpark.functions import col, lit, current_timestamp, when\n\n        # Create Snowpark session\n        try:\n            session = Session.builder.configs(snowflake_config).create()\n        except Exception as e:\n            try:\n                from snowflake.snowpark.context import get_active_session\n                session = get_active_session()\n            except Exception as e:\n                print('FATAL Error : Unable to connect to Snowflake Environment')\n                return False\n\n        # Define table names\n        discovered_urls_table = f\"{table_prefix}_DISCOVERED_URLS\"\n        visited_urls_table = f\"{table_prefix}_VISITED_URLS\"\n        content_hashes_table = f\"{table_prefix}_CONTENT_HASHES\"\n        revisit_schedule_table = f\"{table_prefix}_REVISIT_SCHEDULE\"\n        state_table = f\"{table_prefix}_STATE\"\n\n\n        # Define file paths\n        discovered_urls_file = f\"{table_prefix}_discovered_urls.csv\"\n        visited_urls_file = f\"{table_prefix}_visited_urls.csv\"\n        content_hashes_file = f\"{table_prefix}_content_hashes.csv\"\n        revisit_tracking_file = f\"{table_prefix}_revisit_tracking.csv\"\n        state_file = f\"{table_prefix}_state.json\"\n\n\n        # Create tables if they don't exist\n        create_discovered_urls = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {discovered_urls_table} (\n                    URL STRING PRIMARY KEY,\n                    FOUND_ON STRING,\n                    DEPTH NUMBER,\n                    TIMESTAMP STRING,\n                    CONTENT_HASH STRING,\n                    PAGE_TITLE STRING,\n                    STATUS_CODE NUMBER,\n                    LANGUAGE STRING,\n                    LAST_VISITED TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n                    CONTENT_CHANGED BOOLEAN DEFAULT FALSE,\n                    PREVIOUS_HASH STRING,\n                    VISIT_COUNT NUMBER DEFAULT 1,\n                    NEXT_REVISIT_TIME TIMESTAMP_NTZ,\n                    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n                    UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n                    CLEANED_TEXT VARCHAR,\n                    CONTENT_SIZE NUMBER,\n                    CONTENT_TYPE STRING,\n                    EXTRACTED_LINKS VARIANT,\n                    META_DESCRIPTION STRING,\n                    META_KEYWORDS STRING,\n                    HEADINGS VARIANT,\n                    IMAGES VARIANT,\n                    STRUCTURED_DATA VARIANT\n                )\n                \"\"\"\n\n        create_visited_urls = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {visited_urls_table} (\n                    URL STRING PRIMARY KEY,\n                    VISITED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n                )\n                \"\"\"\n\n        create_content_hashes = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {content_hashes_table} (\n                    CONTENT_HASH STRING PRIMARY KEY,\n                    FIRST_URL STRING,\n                    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n                )\n                \"\"\"\n\n        # Create revisit tracking table if it doesn't exist\n        create_revisit_schedule = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {revisit_schedule_table} (\n                URL STRING PRIMARY KEY,\n                LAST_HASH STRING,\n                LAST_REVISIT TIMESTAMP_NTZ,\n                NEXT_REVISIT TIMESTAMP_NTZ,\n                REVISIT_INTERVAL_HOURS NUMBER DEFAULT 24,\n                CONSECUTIVE_UNCHANGED NUMBER DEFAULT 0,\n                CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n            )\n        \"\"\"\n\n        # Create state table if it doesn't exist\n        create_state_table = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {state_table} (\n            STATE_ID NUMBER IDENTITY PRIMARY KEY,\n            PENDING_URLS VARIANT,\n            PAGES_PROCESSED NUMBER,\n            NEW_URLS_FOUND NUMBER,\n            ERROR_CATEGORIES VARIANT,\n            TIMESTAMP TIMESTAMP_NTZ,\n            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\n        \"\"\"\n        \n\n        # Execute create table statements\n        session.sql(create_revisit_schedule).collect()\n        session.sql(create_discovered_urls).collect()\n        session.sql(create_visited_urls).collect()\n        session.sql(create_content_hashes).collect()\n        session.sql(create_state_table).collect()\n\n        # Upload discovered URLs\n        if os.path.exists(discovered_urls_file):\n            # Read CSV into pandas DataFrame\n            df = pd.read_csv(discovered_urls_file)\n\n            # Define the expected column order matching Snowflake schema\n            expected_columns = [\n                'URL',\n                'FOUND_ON',\n                'DEPTH',\n                'TIMESTAMP',\n                'CONTENT_HASH',\n                'PAGE_TITLE',\n                'STATUS_CODE',\n                'LANGUAGE',\n                'LAST_VISITED',\n                'CONTENT_CHANGED',\n                'PREVIOUS_HASH',\n                'VISIT_COUNT',\n                'NEXT_REVISIT_TIME',\n                'CREATED_AT',\n                'UPDATED_AT',\n                'CLEANED_TEXT',\n                'CONTENT_SIZE',\n                'CONTENT_TYPE',\n                'META_DESCRIPTION',\n                'META_KEYWORDS',\n                'HEADINGS',\n                'EXTRACTED_LINKS',\n                'IMAGES',\n                'STRUCTURED_DATA'\n            ]\n\n            # Convert column names to uppercase\n            df.columns = df.columns.str.upper()\n\n            # Create a new DataFrame with columns in the correct order\n            # Fill missing columns with None/null values\n            mapped_df = pd.DataFrame(columns=expected_columns)\n            for cols in expected_columns:\n                if cols in df.columns:\n                    mapped_df[cols] = df[cols]\n                else:\n                    mapped_df[cols] = None\n\n            # Convert pandas DataFrame to Snowpark DataFrame\n            snow_df = session.create_dataframe(mapped_df)\n\n            # Add missing columns with default values\n            snow_df = snow_df.withColumn(\"LAST_VISITED\", current_timestamp())\n            snow_df = snow_df.withColumn(\"CREATED_AT\", current_timestamp())\n            snow_df = snow_df.withColumn(\"UPDATED_AT\", current_timestamp())\n\n            # Create temp table for merge operation\n            temp_table = \"TEMP_DISCOVERED_URLS\"\n            snow_df.write.mode(\"overwrite\").save_as_table(temp_table)\n\n            # Perform merge operation\n            merge_query = f\"\"\"\n            MERGE INTO {discovered_urls_table} target\n            USING {temp_table} source\n            ON target.URL = source.URL\n            WHEN MATCHED THEN UPDATE SET\n                CONTENT_HASH = source.CONTENT_HASH,\n                LAST_VISITED = CURRENT_TIMESTAMP(),\n                CONTENT_CHANGED = source.CONTENT_CHANGED,\n                VISIT_COUNT = target.VISIT_COUNT + 1,\n                CLEANED_TEXT = source.CLEANED_TEXT,\n                CONTENT_SIZE = source.CONTENT_SIZE,\n                META_DESCRIPTION = source.META_DESCRIPTION,\n                META_KEYWORDS = source.META_KEYWORDS,\n                HEADINGS = source.HEADINGS::VARIANT,\n                EXTRACTED_LINKS = source.EXTRACTED_LINKS::VARIANT,\n                IMAGES = source.IMAGES::VARIANT,\n                STRUCTURED_DATA = source.STRUCTURED_DATA::VARIANT,\n                UPDATED_AT = CURRENT_TIMESTAMP()\n            WHEN NOT MATCHED THEN INSERT (\n                URL, FOUND_ON, DEPTH, TIMESTAMP, CONTENT_HASH, PAGE_TITLE,\n                STATUS_CODE, LANGUAGE, LAST_VISITED, CONTENT_CHANGED,\n                PREVIOUS_HASH, VISIT_COUNT, CLEANED_TEXT, CONTENT_SIZE,\n                CONTENT_TYPE, META_DESCRIPTION, META_KEYWORDS, HEADINGS,\n                EXTRACTED_LINKS, IMAGES, STRUCTURED_DATA\n            ) VALUES (\n                source.URL, source.FOUND_ON, source.DEPTH, source.TIMESTAMP,\n                source.CONTENT_HASH, source.PAGE_TITLE, source.STATUS_CODE,\n                source.LANGUAGE, CURRENT_TIMESTAMP(), source.CONTENT_CHANGED,\n                source.PREVIOUS_HASH, source.VISIT_COUNT, source.CLEANED_TEXT,\n                source.CONTENT_SIZE, source.CONTENT_TYPE, source.META_DESCRIPTION,\n                source.META_KEYWORDS, source.HEADINGS::VARIANT, source.EXTRACTED_LINKS::VARIANT,\n                source.IMAGES::VARIANT, source.STRUCTURED_DATA::VARIANT\n            )\n            \"\"\"\n            session.sql(merge_query).collect()\n            session.sql(f\"DROP TABLE IF EXISTS {temp_table}\").collect()\n            print(f\"✓ Merged {len(df)} rows into {discovered_urls_table}\")\n\n        # Upload visited URLs\n        if os.path.exists(visited_urls_file):\n            df = pd.read_csv(visited_urls_file)\n            df.columns = df.columns.str.upper()\n\n            snow_df = session.create_dataframe(df)\n\n            temp_table = \"TEMP_VISITED_URLS\"\n            snow_df.write.mode(\"overwrite\").save_as_table(temp_table)\n\n            merge_query = f\"\"\"\n            MERGE INTO {visited_urls_table} target\n            USING {temp_table} source\n            ON target.URL = source.URL\n            WHEN NOT MATCHED THEN INSERT (URL, VISITED_AT)\n            VALUES (source.URL, CURRENT_TIMESTAMP())\n            \"\"\"\n            session.sql(merge_query).collect()\n            session.sql(f\"DROP TABLE IF EXISTS {temp_table}\").collect()\n            print(f\"✓ Merged {len(df)} rows into {visited_urls_table}\")\n\n        # Upload content hashes\n        if os.path.exists(content_hashes_file):\n            df = pd.read_csv(content_hashes_file)\n            df.columns = df.columns.str.upper()\n\n            snow_df = session.create_dataframe(df)\n\n            temp_table = \"TEMP_CONTENT_HASHES\"\n            snow_df.write.mode(\"overwrite\").save_as_table(temp_table)\n\n            merge_query = f\"\"\"\n            MERGE INTO {content_hashes_table} target\n            USING {temp_table} source\n            ON target.CONTENT_HASH = source.CONTENT_HASH\n            WHEN NOT MATCHED THEN INSERT (CONTENT_HASH, FIRST_URL, CREATED_AT)\n            VALUES (source.CONTENT_HASH, source.FIRST_URL, CURRENT_TIMESTAMP())\n            \"\"\"\n            session.sql(merge_query).collect()\n            session.sql(f\"DROP TABLE IF EXISTS {temp_table}\").collect()\n            print(f\"✓ Merged {len(df)} rows into {content_hashes_table}\")\n\n        # Upload revisit tracking data\n        if os.path.exists(revisit_tracking_file):\n            df = pd.read_csv(revisit_tracking_file)\n            df.columns = df.columns.str.upper()\n\n            # Convert pandas DataFrame to Snowpark DataFrame\n            snow_df = session.create_dataframe(df)\n\n            # Create temp table for merge operation\n            temp_table = \"TEMP_REVISIT_SCHEDULE\"\n            snow_df.write.mode(\"overwrite\").save_as_table(temp_table)\n\n            # Perform merge operation\n            merge_query = f\"\"\"\n            MERGE INTO {revisit_schedule_table} target\n            USING {temp_table} source\n            ON target.URL = source.URL\n            WHEN MATCHED THEN UPDATE SET\n                LAST_HASH = source.LAST_HASH,\n                LAST_REVISIT = source.LAST_REVISIT::TIMESTAMP_NTZ,\n                NEXT_REVISIT = source.NEXT_REVISIT::TIMESTAMP_NTZ,\n                REVISIT_INTERVAL_HOURS = source.REVISIT_INTERVAL_HOURS,\n                CONSECUTIVE_UNCHANGED = source.CONSECUTIVE_UNCHANGED,\n                CREATED_AT = CURRENT_TIMESTAMP()\n            WHEN NOT MATCHED THEN INSERT (\n                URL, LAST_HASH, LAST_REVISIT, NEXT_REVISIT,\n                REVISIT_INTERVAL_HOURS, CONSECUTIVE_UNCHANGED\n            ) VALUES (\n                source.URL, source.LAST_HASH,\n                source.LAST_REVISIT::TIMESTAMP_NTZ,\n                source.NEXT_REVISIT::TIMESTAMP_NTZ,\n                source.REVISIT_INTERVAL_HOURS,\n                source.CONSECUTIVE_UNCHANGED\n            )\n            \"\"\"\n            session.sql(merge_query).collect()\n            session.sql(f\"DROP TABLE IF EXISTS {temp_table}\").collect()\n            print(f\"✓ Merged {len(df)} rows into {revisit_schedule_table}\")\n\n        # Upload state data if file exists\n        if os.path.exists(state_file):\n            try:\n                with open(state_file, 'r') as f:\n                    state_data = json.load(f)\n                \n                # Format state data for Snowflake\n                formatted_state = {\n                    'PENDING_URLS': json.dumps(state_data.get('pending_urls', [])),\n                    'PAGES_PROCESSED': state_data.get('pages_processed', 0),\n                    'NEW_URLS_FOUND': state_data.get('new_urls_found', 0),\n                    'ERROR_CATEGORIES': json.dumps(state_data.get('error_categories', {})),\n                    'TIMESTAMP': state_data.get('timestamp')\n                }\n                \n                # Convert arrays and objects to VARIANT type using TO_VARIANT\n                # Insert new state using direct value insertion\n                insert_query = f\"\"\"\n                INSERT OVERWRITE INTO {state_table} (\n                    PENDING_URLS,\n                    PAGES_PROCESSED,\n                    NEW_URLS_FOUND,\n                    ERROR_CATEGORIES,\n                    TIMESTAMP\n                )\n                SELECT\n                    PARSE_JSON('{formatted_state['PENDING_URLS']}'),\n                    {formatted_state['PAGES_PROCESSED']},\n                    {formatted_state['NEW_URLS_FOUND']},\n                    PARSE_JSON('{formatted_state['ERROR_CATEGORIES']}'),\n                    '{formatted_state['TIMESTAMP']}'\n                \"\"\"\n                \n                session.sql(insert_query).collect()\n                print(f\"✓ Uploaded state data to {state_table}\")\n\n            except Exception as e:\n                print(f\"Error uploading state data: {e}\")\n\n        session.close()\n        return True\n\n    except Exception as e:\n        print(f\"Error uploading CSV files to Snowflake: {str(e)}\")\n        if 'session' in locals():\n            session.close()\n        return False\n\n\nclass TelemetryManager:\n    \"\"\"Enhanced telemetry manager with health monitoring.\"\"\"\n\n    def __init__(self, report_interval=15, killer=None):\n        self.report_interval = report_interval\n        self.start_time = None\n        self.last_report_time = None\n        self.running = True\n        self.killer = killer\n\n        # Enhanced telemetry data\n        self.pages_processed = 0\n        self.urls_discovered = 0\n        self.errors_encountered = 0\n        self.queue_size = 0\n        self.active_threads = 0\n        self.bytes_downloaded = 0\n        self.avg_response_time = 0.0\n        self.response_times = deque(maxlen=100)  # Use deque with maxlen\n        self.pages_per_depth = defaultdict(int)\n        self.status_codes = defaultdict(int)\n        self.domains_crawled = set()\n        self.non_english_pages_skipped = 0\n        self.english_pages_processed = 0\n        self.url_language_filtered = 0\n        self.content_changes_detected = 0\n        self.revisited_pages = 0\n        self.duplicate_content_skipped = 0\n\n        # Error categorization\n        self.error_categories = defaultdict(int)\n        self.recoverable_errors = 0\n        self.permanent_errors = 0\n\n        # Health monitoring\n        self.memory_monitor = MemoryMonitor()\n        self.last_health_check = time.time()\n\n        # Thread synchronization\n        self.telemetry_lock = threading.RLock()\n        self.telemetry_thread = None\n\n        # Setup telemetry logging\n        self.telemetry_logger = logging.getLogger('telemetry')\n        telemetry_handler = logging.FileHandler('crawler_telemetry.log')\n        telemetry_formatter = logging.Formatter('%(asctime)s - TELEMETRY - %(message)s')\n        telemetry_handler.setFormatter(telemetry_formatter)\n        self.telemetry_logger.addHandler(telemetry_handler)\n        self.telemetry_logger.setLevel(logging.ERROR)\n\n    def start_telemetry(self):\n        \"\"\"Start the telemetry reporting thread.\"\"\"\n        self.start_time = time.time()\n        self.last_report_time = self.start_time\n        self.telemetry_thread = threading.Thread(target=self._telemetry_worker, name=\"Telemetry\")\n        self.telemetry_thread.daemon = True\n        self.telemetry_thread.start()\n\n    def stop_telemetry(self):\n        \"\"\"Stop the telemetry reporting.\"\"\"\n        self.running = False\n        if self.telemetry_thread:\n            self.telemetry_thread.join(timeout=2)\n\n    def update_page_processed(self, depth: int, response_time: float, status_code: int, domain: str, content_size: int,\n                              is_english: bool, is_revisit: bool = False, content_changed: bool = False,\n                              is_duplicate: bool = False):\n        \"\"\"Enhanced page processing update with health monitoring.\"\"\"\n        with self.telemetry_lock:\n            self.pages_processed += 1\n            self.pages_per_depth[depth] += 1\n            self.status_codes[status_code] += 1\n            self.domains_crawled.add(domain)\n            self.bytes_downloaded += content_size\n\n            if is_revisit:\n                self.revisited_pages += 1\n\n            if content_changed:\n                self.content_changes_detected += 1\n\n            if is_duplicate:\n                self.duplicate_content_skipped += 1\n\n            if is_english:\n                self.english_pages_processed += 1\n            else:\n                self.non_english_pages_skipped += 1\n\n            # Update response time statistics with bounded collection\n            self.response_times.append(response_time)\n            self.avg_response_time = sum(self.response_times) / len(self.response_times)\n\n            # Health check\n            current_time = time.time()\n            if current_time - self.last_health_check > 60:  # Check every minute\n                self._perform_health_check()\n                self.last_health_check = current_time\n\n    def update_error_count(self, error_type: str = \"general\", is_recoverable: bool = True):\n        \"\"\"Enhanced error tracking with categorization.\"\"\"\n        with self.telemetry_lock:\n            self.errors_encountered += 1\n            self.error_categories[error_type] += 1\n\n            if is_recoverable:\n                self.recoverable_errors += 1\n            else:\n                self.permanent_errors += 1\n\n    def _perform_health_check(self):\n        \"\"\"Perform health checks and log warnings.\"\"\"\n        try:\n            # Memory check\n            memory_usage = self.memory_monitor.get_memory_usage_mb()\n            if memory_usage > 512:  # Warn if over 512MB\n                logging.warning(f\"High memory usage: {memory_usage:.2f} MB\")\n\n            # Error rate check\n            if self.pages_processed > 0:\n                error_rate = (self.errors_encountered / self.pages_processed) * 100\n                if error_rate > 10:  # Warn if error rate > 10%\n                    logging.warning(f\"High error rate: {error_rate:.1f}%\")\n\n            # Response time check\n            if self.avg_response_time > 5.0:  # Warn if avg response time > 5s\n                logging.warning(f\"Slow response times: {self.avg_response_time:.2f}s average\")\n\n        except Exception as e:\n            logging.error(f\"Health check failed: {e}\")\n\n    def update_urls_discovered(self, count: int):\n        \"\"\"Update the count of discovered URLs.\"\"\"\n        with self.telemetry_lock:\n            self.urls_discovered += count\n\n    def update_url_language_filtered(self):\n        \"\"\"Increment URL language filtered counter.\"\"\"\n        with self.telemetry_lock:\n            self.url_language_filtered += 1\n\n    def update_queue_size(self, size: int):\n        \"\"\"Update current queue size.\"\"\"\n        with self.telemetry_lock:\n            self.queue_size = size\n\n    def update_active_threads(self, count: int):\n        \"\"\"Update active thread count.\"\"\"\n        with self.telemetry_lock:\n            self.active_threads = count\n\n    def _telemetry_worker(self):\n        \"\"\"Worker thread for periodic telemetry reporting.\"\"\"\n        while self.running and (not self.killer or not self.killer.should_stop()):\n            try:\n                time.sleep(self.report_interval)\n                if self.running and (not self.killer or not self.killer.should_stop()):\n                    self._generate_progress_report()\n            except Exception as e:\n                logging.error(f\"Telemetry worker error: {e}\")\n\n    def _generate_progress_report(self):\n        \"\"\"Generate enhanced progress report with health metrics.\"\"\"\n        current_time = time.time()\n\n        with self.telemetry_lock:\n            elapsed_time = current_time - (self.start_time or current_time)\n            time_since_last_report = current_time - (self.last_report_time or current_time)\n\n            # Calculate rates\n            pages_per_second = self.pages_processed / elapsed_time if elapsed_time > 0 else 0\n            pages_per_second_recent = 0\n            if hasattr(self, '_last_pages_processed'):\n                pages_since_last = self.pages_processed - self._last_pages_processed\n                pages_per_second_recent = pages_since_last / time_since_last_report if time_since_last_report > 0 else 0\n\n            urls_per_second = self.urls_discovered / elapsed_time if elapsed_time > 0 else 0\n            mb_downloaded = self.bytes_downloaded / (1024 * 1024)\n            mb_per_second = mb_downloaded / elapsed_time if elapsed_time > 0 else 0\n\n            # Memory usage\n            memory_usage = self.memory_monitor.get_memory_usage_mb()\n\n            # Create enhanced progress report\n            report = self._format_progress_report(\n                elapsed_time, pages_per_second, pages_per_second_recent,\n                urls_per_second, mb_downloaded, mb_per_second, memory_usage\n            )\n\n            # Display and log report\n            print(\"\\n\" + \"=\" * 80)\n            print(report)\n            print(\"=\" * 80 + \"\\n\")\n\n            # Enhanced telemetry logging\n            self.telemetry_logger.info(\n                f\"Pages: {self.pages_processed}, English: {self.english_pages_processed}, \"\n                f\"URLs: {self.urls_discovered}, Errors: {self.errors_encountered}, \"\n                f\"Queue: {self.queue_size}, Rate: {pages_per_second:.2f} p/s, \"\n                f\"Data: {mb_downloaded:.2f} MB, Memory: {memory_usage:.2f} MB, \"\n                f\"Revisited: {self.revisited_pages}, Changes: {self.content_changes_detected}, \"\n                f\"Duplicates: {self.duplicate_content_skipped}\"\n            )\n\n            # Update tracking variables\n            self._last_pages_processed = self.pages_processed\n            self.last_report_time = current_time\n\n    def _format_progress_report(self, elapsed_time, pages_per_second, pages_per_second_recent,\n                                urls_per_second, mb_downloaded, mb_per_second, memory_usage):\n        \"\"\"Format enhanced progress report with health metrics.\"\"\"\n        import datetime\n\n        hours, remainder = divmod(elapsed_time, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        # Build depth distribution\n        depth_info = \", \".join([f\"D{d}: {count}\" for d, count in sorted(self.pages_per_depth.items())])\n\n        # Build status code distribution\n        status_info = \", \".join([f\"{code}: {count}\" for code, count in sorted(self.status_codes.items())])\n\n        # Build error category distribution\n        error_info = \", \".join([f\"{cat}: {count}\" for cat, count in sorted(self.error_categories.items())])\n\n        # Calculate efficiency metrics\n        error_rate = (self.errors_encountered / max(self.pages_processed, 1)) * 100\n        english_rate = (self.english_pages_processed / max(self.pages_processed, 1)) * 100\n        change_rate = (self.content_changes_detected / max(self.revisited_pages, 1)) * 100\n        duplicate_rate = (self.duplicate_content_skipped / max(self.pages_processed, 1)) * 100\n\n        report = f\"\"\"\n🕐 ENHANCED INCREMENTAL CRAWLER PROGRESS REPORT - {datetime.datetime.now().strftime('%H:%M:%S')}\n\n⏱️  Runtime: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\n📊 Pages Processed: {self.pages_processed:,} ({pages_per_second:.2f}/sec avg, {pages_per_second_recent:.2f}/sec recent)\n🔄 Pages Revisited: {self.revisited_pages:,}\n🔄 Content Changes: {self.content_changes_detected:,} ({change_rate:.1f}% of revisited)\n🔗 Duplicate Content: {self.duplicate_content_skipped:,} ({duplicate_rate:.1f}% of total)\n🇬🇧 English Pages: {self.english_pages_processed:,} ({english_rate:.1f}% of total)\n🌐 Non-English Skipped: {self.non_english_pages_skipped:,} (content filtering)\n🔗 URL Language Filtered: {self.url_language_filtered:,} (URL path filtering)\n🔗 URLs Discovered: {self.urls_discovered:,} ({urls_per_second:.2f}/sec)\n❌ Errors: {self.errors_encountered:,} ({error_rate:.1f}% error rate)\n📥 Queue Size: {self.queue_size:,}\n🧵 Active Threads: {self.active_threads}\n\n📈 Performance Metrics:\n   • Average Response Time: {self.avg_response_time:.3f}s\n   • Data Downloaded: {mb_downloaded:.2f} MB ({mb_per_second:.3f} MB/s)\n   • Memory Usage: {memory_usage:.2f} MB\n   • Domains Crawled: {len(self.domains_crawled)}\n\n🔍 Error Analysis:\n   • Recoverable: {self.recoverable_errors}, Permanent: {self.permanent_errors}\n   • Categories: {error_info}\n\n📊 Depth Distribution: {depth_info}\n🌐 Status Codes: {status_info}\n\n💡 Press Ctrl+C to stop gracefully or call crawler.stop() programmatically\n        \"\"\".strip()\n\n        return report\n\n    def get_final_report(self):\n        \"\"\"Generate final telemetry report.\"\"\"\n        if not self.start_time:\n            return \"No telemetry data available\"\n\n        total_time = time.time() - self.start_time\n        memory_usage = self.memory_monitor.get_memory_usage_mb()\n\n        return self._format_progress_report(\n            total_time,\n            self.pages_processed / total_time if total_time > 0 else 0,\n            0,\n            self.urls_discovered / total_time if total_time > 0 else 0,\n            self.bytes_downloaded / (1024 * 1024),\n            (self.bytes_downloaded / (1024 * 1024)) / total_time if total_time > 0 else 0,\n            memory_usage\n        )\n\n\nclass ThreadSafeCrawler:\n    def __init__(self, config: Dict):\n        \"\"\"Initialize the enhanced multi-threaded crawler with comprehensive improvements.\"\"\"\n        self.config = config\n\n        # Initialize graceful shutdown handler with timeout\n        self.killer = GracefulKiller(timeout_hours=config.get('max_runtime_hours'))\n\n        # Initialize URL validator\n        self.url_validator = URLValidator()\n\n        # Initialize storage based on mode\n        self.use_database = config.get('use_database', False)\n        self.table_prefix = config.get('table_prefix', 'CRAWLER')\n\n        if not self.use_database:\n            # CSV Storage configuration\n            csv_config = {}\n            self.storage = CSVStorage(**csv_config)\n            print(\"Using CSV file storage with content capabilities\")\n\n        # Thread-safe data structures\n        self.urls_to_visit: Queue = Queue()\n        self.running = True\n        self.new_urls_found = 0\n        self.pages_since_last_new = 0\n\n        # Thread synchronization with RLock for reentrant safety\n        self.stats_lock = threading.RLock()\n        self.session_lock = threading.RLock()\n\n        # Enhanced configuration with validation\n        self.max_depth = max(1, config.get('max_depth', 3))\n        self.allowed_domains = set(config.get('allowed_domains', []))\n        self.starting_urls = config.get('starting_urls', [])\n        self.diminishing_returns_threshold = config.get('diminishing_returns_threshold', 10)\n        self.diminishing_returns_pages = config.get('diminishing_returns_pages', 20)\n        self.request_delay = max(0.1, config.get('request_delay', 1.0))\n        self.timeout = max(5, config.get('timeout', 10))\n        self.max_retries = max(1, config.get('max_retries', 3))\n        self.enable_language_filtering = config.get('enable_language_filtering', True)\n        self.enable_url_language_filtering = config.get('enable_url_language_filtering', True)\n        self.max_workers = max(1, min(10, config.get('max_workers', 5)))\n        self.queue_timeout = max(1, config.get('queue_timeout', 5))\n        self.save_interval = max(10, config.get('save_interval', 50))\n        self.telemetry_interval = max(5, config.get('telemetry_interval', 15))\n        self.export_csv = config.get('export_csv', True)\n        self.debug_mode = config.get('debug_mode', False)\n\n        # Enhanced incremental crawling options\n        self.enable_content_change_detection = config.get('enable_content_change_detection', True)\n        self.revisit_interval_hours = max(1, config.get('revisit_interval_hours', 24))\n        self.max_revisit_urls_per_run = max(10, config.get('max_revisit_urls_per_run', 50))\n        self.content_change_threshold = max(0.01, config.get('content_change_threshold', 0.1))\n        self.force_revisit_depth = max(0, config.get('force_revisit_depth', 2))\n\n        # Content storage configuration\n        self.store_raw_html = config.get('store_raw_html', True)\n        self.store_cleaned_text = config.get('store_cleaned_text', True)\n        self.store_extracted_data = config.get('store_extracted_data', True)\n        self.max_content_size = config.get('max_content_size', 1000000)  # 1MB limit\n        self.extract_metadata = config.get('extract_metadata', True)\n        self.extract_headings = config.get('extract_headings', True)\n        self.extract_images = config.get('extract_images', True)\n        self.extract_links = config.get('extract_links', True)\n        self.extract_structured_data = config.get('extract_structured_data', True)\n\n        # Initialize enhanced telemetry\n        self.telemetry = TelemetryManager(self.telemetry_interval, self.killer)\n\n        # Thread-local session storage with enhanced management\n        self.session_storage = threading.local()\n        self.session_pool = weakref.WeakSet()\n\n        # Statistics tracking with thread safety\n        self.pages_processed = 0\n        self.errors_encountered = 0\n        self.start_time = None\n        self._last_save = None\n\n        # Error categorization\n        self.error_categories = defaultdict(int)\n        self.error_lock = threading.Lock()\n\n        # Setup enhanced logging with thread safety\n        log_level = logging.ERROR if self.debug_mode else logging.INFO\n        self._setup_logging(log_level)\n\n        # Memory monitoring\n        self.memory_monitor = MemoryMonitor()\n\n        # Load previous state\n        self._load_state()\n\n        # Initialize starting URLs with enhanced incremental support\n        self._initialize_starting_urls()\n\n    def _setup_logging(self, log_level):\n        \"\"\"Setup thread-safe logging configuration.\"\"\"\n        # Clear any existing handlers\n        root = logging.getLogger()\n        if root.handlers:\n            for handler in root.handlers:\n                root.removeHandler(handler)\n\n        logging.basicConfig(\n            level=log_level,\n            format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('crawler_debug.log'),\n                # logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def _process_webpage_content(self, html_content: str, url: str) -> Dict:\n        \"\"\"Optimized content processing with size checks.\"\"\"\n        # Early size check\n        content_size = len(html_content)\n        if content_size > self.max_content_size:\n            return {\n                'cleaned_text': '',\n                'meta_description': '',\n                'meta_keywords': '',\n                'headings': {},\n                'images': [],\n                'extracted_links': [],\n                'structured_data': {}\n            }\n\n        try:\n            soup = BeautifulSoup(html_content, 'html.parser',\n                                 parse_only=SoupStrainer(['meta', 'title', 'a', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']))\n\n            # Process only essential content\n            meta_description = \"\"\n            meta_keywords = \"\"\n            if self.extract_metadata:\n                meta_desc = soup.find('meta', attrs={'name': 'description'})\n                meta_description = meta_desc.get('content', '')[:500] if meta_desc else \"\"\n\n                meta_key = soup.find('meta', attrs={'name': 'keywords'})\n                meta_keywords = meta_key.get('content', '')[:500] if meta_key else \"\"\n\n            # Extract headings more efficiently\n            headings = {}\n            if self.extract_headings:\n                for i in range(1, 7):\n                    heading_tags = soup.find_all(f'h{i}', limit=10)\n                    if heading_tags:\n                        headings[f'h{i}'] = [tag.get_text().strip()[:200] for tag in heading_tags]\n\n            # Optimize link extraction\n            extracted_links = []\n            if self.extract_links:\n                seen_urls = set()\n                for link in soup.find_all('a', href=True, limit=100):\n                    href = link.get('href', '')\n                    if href and href not in seen_urls:\n                        absolute_url = urljoin(url, href)\n                        if self._is_valid_https_url(absolute_url):\n                            extracted_links.append(absolute_url)\n                            seen_urls.add(href)\n\n            # Simplified content cleaning\n            cleaned_text = ' '.join(soup.stripped_strings)[:50000] if self.store_cleaned_text else \"\"\n\n            return {\n                'cleaned_text': cleaned_text,\n                'meta_description': meta_description,\n                'meta_keywords': meta_keywords,\n                'headings': headings,\n                'images': [],  # Simplified image processing\n                'extracted_links': extracted_links,\n                'structured_data': {}  # Simplified structured data\n            }\n\n        except Exception as e:\n            logging.error(f\"Error processing content for {url}: {e}\")\n            return {\n                'cleaned_text': '',\n                'meta_description': '',\n                'meta_keywords': '',\n                'headings': {},\n                'images': [],\n                'extracted_links': [],\n                'structured_data': {}\n            }\n\n    def _get_enhanced_content_hash(self, content: str) -> str:\n        \"\"\"Generate enhanced hash focusing on meaningful content only.\"\"\"\n        try:\n            if not content or not isinstance(content, str):\n                return hashlib.md5(b'').hexdigest()\n\n            # Parse HTML with error handling\n            soup = BeautifulSoup(content, 'html.parser')\n            if not soup:\n                return hashlib.md5(content.encode('utf-8')).hexdigest()\n\n            # Remove dynamic/irrelevant elements\n            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n                tag.decompose()\n\n            # Remove comments\n            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n                comment.extract()\n\n            # Focus on main content areas\n            main_content = \"\"\n            content_selectors = [\n                'main', 'article', '[role=\"main\"]',\n                '.content', '.post', '.entry',\n                '#content', '#main', '#post'\n            ]\n\n            for selector in content_selectors:\n                elements = soup.select(selector)\n                if elements:\n                    main_content = ' '.join(elem.get_text() for elem in elements)\n                    break\n\n            # Fallback to body if no main content found\n            if not main_content:\n                body = soup.find('body')\n                main_content = body.get_text() if body else soup.get_text()\n\n            # Clean and normalize text\n            cleaned = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', main_content)\n            cleaned = re.sub(r'\\d{1,2}:\\d{2}(:\\d{2})?', '', cleaned)\n            cleaned = re.sub(r'\\b\\d+\\s+(views?|comments?|likes?|shares?)\\b', '', cleaned, flags=re.IGNORECASE)\n            cleaned = re.sub(r'\\bposted\\s+\\d+\\s+(minutes?|hours?|days?)\\s+ago\\b', '', cleaned, flags=re.IGNORECASE)\n\n            # Normalize whitespace\n            cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n\n            # Generate hash of normalized content\n            return hashlib.sha256(cleaned.encode('utf-8')).hexdigest()\n\n        except Exception as e:\n            self.logger.error(f\"Error generating enhanced content hash: {e}\")\n            return hashlib.md5(content.encode('utf-8')).hexdigest()\n\n    def _initialize_starting_urls(self):\n        \"\"\"Initialize queue with starting URLs and URLs due for revisit.\"\"\"\n        urls_added = 0\n\n        # 1. Add URLs due for revisit\n        if self.enable_content_change_detection and hasattr(self.storage, 'get_urls_for_revisit'):\n            try:\n                revisit_urls = self.storage.get_urls_for_revisit(\n                    self.revisit_interval_hours,\n                    self.max_revisit_urls_per_run\n                )\n\n                for url, found_on, depth in revisit_urls:\n                    if self.url_validator.is_valid_https_url(url):\n                        self.urls_to_visit.put((url, found_on, depth, True))  # True = is_revisit\n                        urls_added += 1\n\n                self.logger.info(f\"Added {len(revisit_urls)} URLs for revisit\")\n            except Exception as e:\n                self.logger.error(f\"Error loading revisit URLs: {e}\")\n\n        # 2. Add pending URLs from previous session\n        try:\n            state = self.storage.load_crawler_state()\n            pending_urls = state.get('pending_urls', [])\n\n            for url_data in pending_urls:\n                if isinstance(url_data, (list, tuple)) and len(url_data) >= 3:\n                    url, found_on, depth = url_data[0], url_data[1], url_data[2]\n                    if self.url_validator.is_valid_https_url(url) and not self.storage.is_url_visited(url):\n                        self.urls_to_visit.put((url, found_on, depth, False))  # False = not revisit\n                        urls_added += 1\n        except Exception as e:\n            self.logger.error(f\"Error loading pending URLs: {e}\")\n\n        # 3. Add starting URLs if needed\n        if urls_added < 5:\n            for url in self.starting_urls:\n                if self.url_validator.is_valid_https_url(url):\n                    self.urls_to_visit.put((url, \"\", 0, False))\n                    urls_added += 1\n\n        # 4. Find unvisited discovered URLs\n        if urls_added == 0:\n            unvisited_urls = self._find_unvisited_discovered_urls()\n            for url_data in unvisited_urls[:50]:\n                if isinstance(url_data, (list, tuple)) and len(url_data) >= 3:\n                    self.urls_to_visit.put((*url_data, False))  # Add revisit flag\n                    urls_added += 1\n\n        self.logger.info(f\"Queue initialized with {urls_added} URLs (including revisits)\")\n\n    def _is_valid_https_url(self, url: str) -> bool:\n        \"\"\"Enhanced URL validation with security checks.\"\"\"\n        return self.url_validator.is_valid_https_url(url)\n\n    def _is_english_url(self, url: str) -> bool:\n        \"\"\"Check if URL indicates English content.\"\"\"\n        if not self.enable_url_language_filtering:\n            return True\n\n        try:\n            # Check if URL is in allowed domains\n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n\n            if self.allowed_domains:\n                domain_allowed = any(allowed in domain for allowed in self.allowed_domains)\n                if not domain_allowed:\n                    return False\n\n            # Check URL language patterns\n            detected_lang = detect_language_from_url_path(url)\n            return detected_lang in [None, 'en']\n\n        except Exception as e:\n            self.logger.debug(f\"Error checking URL language for {url}: {e}\")\n            return True\n\n    def _detect_language(self, text: str) -> str:\n        \"\"\"Enhanced language detection with error handling.\"\"\"\n        if not text or len(text.strip()) < 50:\n            return 'en'  # Default to English for short content\n\n        try:\n            # Extract text content for language detection\n            soup = BeautifulSoup(text, 'html.parser')\n            if soup:\n                # Remove script and style elements\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n                text_content = soup.get_text()\n            else:\n                text_content = text\n\n            # Clean text for language detection\n            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n\n            if len(text_content) < 50:\n                return 'en'\n\n            # Use langdetect with confidence threshold\n            detected = detect(text_content[:1000])  # Use first 1000 chars for speed\n            return detected if detected else 'en'\n\n        except Exception as e:\n            self.logger.debug(f\"Language detection failed: {e}\")\n            return 'en'\n\n    def _normalize_url(self, url: str) -> Optional[str]:\n        \"\"\"Enhanced URL normalization with error handling.\"\"\"\n        if not url or not isinstance(url, str):\n            return None\n\n        try:\n            # Parse URL\n            parsed = urlparse(url.strip())\n\n            if not parsed.scheme or not parsed.netloc:\n                return None\n\n            # Normalize components\n            scheme = parsed.scheme.lower()\n            netloc = parsed.netloc.lower()\n            path = parsed.path.rstrip('/')\n\n            # Remove default ports\n            if ':80' in netloc and scheme == 'http':\n                netloc = netloc.replace(':80', '')\n            elif ':443' in netloc and scheme == 'https':\n                netloc = netloc.replace(':443', '')\n\n            # Reconstruct URL\n            normalized = urlunparse((scheme, netloc, path, '', '', ''))\n            return normalized\n\n        except Exception as e:\n            self.logger.debug(f\"Error normalizing URL {url}: {e}\")\n            return None\n\n    def _normalize_url_with_language(self, url: str) -> str:\n        \"\"\"\n        Normalize URL by handling language path segments and .html extensions.\n        Examples:\n        - https://docs.snowflake.com/en/user-guide -> https://docs.snowflake.com/user-guide\n        - https://docs.snowflake.com/en/user-guide.html -> https://docs.snowflake.com/user-guide\n        - https://docs.snowflake.com/user-guide.html -> https://docs.snowflake.com/user-guide\n        \"\"\"\n        if not url:\n            return url\n\n        try:\n            parsed = urlparse(url)\n            path_parts = parsed.path.strip('/').split('/')\n\n            # Common language codes in URLs\n            language_codes = {'en', 'fr', 'de', 'es', 'it', 'ja', 'ko', 'pt', 'zh', 'ru'}\n\n            # If first path segment is a language code, remove it\n            if path_parts and path_parts[0].lower() in language_codes:\n                path_parts = path_parts[1:]\n\n            # Remove .html extension from the last path segment\n            if path_parts and path_parts[-1].lower().endswith('.html'):\n                path_parts[-1] = path_parts[-1][:-5]  # Remove .html\n\n            # Reconstruct URL without language code and .html\n            new_path = '/' + '/'.join(path_parts)\n            if new_path == '/':\n                new_path = ''\n\n            normalized = urlunparse((\n                parsed.scheme.lower(),\n                parsed.netloc.lower(),\n                new_path,\n                parsed.params,\n                parsed.query,\n                ''  # Remove fragments\n            ))\n\n            return normalized\n\n        except Exception as e:\n            logging.error(f\"Error normalizing URL {url}: {e}\")\n            return url\n\n    def _process_single_url(self, url_data: Tuple[str, str, int, bool]) -> List[Tuple[str, str, int]]:\n        \"\"\"Process a single URL with enhanced content extraction and storage.\"\"\"\n        import datetime\n\n        if len(url_data) == 4:\n            current_url, found_on, depth, is_revisit = url_data\n        else:\n            current_url, found_on, depth = url_data\n            is_revisit = False\n\n        new_urls = []\n\n        try:\n            # Check if we should stop\n            if self.killer.should_stop():\n                return new_urls\n\n            # Check depth limit\n            if depth > self.max_depth:\n                return new_urls\n\n            # Normalize the current URL\n            normalized_url = self._normalize_url_with_language(current_url)\n\n            # Check if normalized URL exists before processing\n            if self.storage.url_exists(normalized_url):\n                return new_urls\n\n            # For revisits, skip the \"already visited\" check\n            if not is_revisit and self.storage.is_url_visited(current_url):\n                return new_urls\n\n            # URL language check\n            if not self._is_english_url(current_url):\n                self.telemetry.update_url_language_filtered()\n                return new_urls\n\n            self.logger.info(f\"{'Revisiting' if is_revisit else 'Crawling'} (depth {depth}): {current_url}\")\n\n            # Fetch page content\n            page_data = self._fetch_page(current_url)\n            if not page_data:\n                return new_urls\n\n            html_content, page_title, status_code, response_time, content_size, detected_language = page_data\n\n            # Check content size limit\n            if content_size > self.max_content_size:\n                self.logger.warning(f\"Content too large ({content_size} bytes), skipping: {current_url}\")\n                return new_urls\n\n            # Process webpage content for storage\n            content_data = self._process_webpage_content(html_content, current_url)\n\n            # Generate enhanced content hash\n            content_hash = self._get_enhanced_content_hash(html_content)\n\n            # Add content hash to storage\n            self.storage.add_content_hash(content_hash, current_url)\n\n            # Check for content changes and duplicates\n            content_changed = True\n            previous_hash = \"\"\n            is_duplicate = False\n\n            if self.enable_content_change_detection:\n                content_changed, previous_hash = self.storage.check_content_change(current_url, content_hash)\n                is_duplicate, canonical_url = self.storage.check_duplicate_content(current_url, content_hash)\n\n                if is_revisit and not content_changed:\n                    self.logger.info(f\"No content change detected for: {current_url}\")\n                    self.storage.mark_url_visited(current_url)\n                    domain = urlparse(current_url).netloc\n                    is_english = detected_language == 'en'\n                    self.telemetry.update_page_processed(depth, response_time, status_code, domain,\n                                                         content_size, is_english, is_revisit, content_changed)\n                    return new_urls\n\n                if is_duplicate:\n                    self.logger.info(f\"Duplicate content detected: {current_url} -> {canonical_url}\")\n                    self.storage.mark_url_visited(current_url)\n                    domain = urlparse(current_url).netloc\n                    is_english = detected_language == 'en'\n                    self.telemetry.update_page_processed(depth, response_time, status_code, domain,\n                                                         content_size, is_english, is_revisit, content_changed,\n                                                         True)\n                    return new_urls\n\n            # Content language check\n            is_english = detected_language == 'en'\n            if self.enable_language_filtering and not is_english:\n                return new_urls\n\n            # Create enhanced crawl result with content\n            result = CrawlResult(\n                url=current_url,\n                found_on=found_on,\n                depth=depth,\n                timestamp=datetime.datetime.now().isoformat(),\n                content_hash=content_hash,\n                page_title=page_title,\n                status_code=status_code,\n                language=detected_language,\n                last_visited=datetime.datetime.now().isoformat(),\n                content_changed=content_changed,\n                previous_hash=previous_hash,\n                raw_html=html_content[:100000] if self.store_raw_html else \"\",\n                cleaned_text=content_data['cleaned_text'],\n                content_size=content_size,\n                content_type='text/html',\n                extracted_links=content_data['extracted_links'],\n                meta_description=content_data['meta_description'],\n                meta_keywords=content_data['meta_keywords'],\n                headings=content_data['headings'],\n                images=content_data['images'],\n                structured_data=content_data['structured_data']\n            )\n\n            # Store the enhanced result\n            if hasattr(self.storage, 'insert_discovered_url_with_content'):\n                # print('************Sunny insert_discovered_url_with_content**********')\n                self.storage.insert_discovered_url_with_content(result)\n            else:\n                # Fallback to regular storage\n                is_new_url, is_unique_content = self.storage.insert_discovered_url(result)\n                self._update_stats(is_unique_content)\n\n            self.storage.mark_url_visited(current_url)\n            domain = urlparse(current_url).netloc\n            self.telemetry.update_page_processed(depth, response_time, status_code, domain,\n                                                 content_size, is_english, is_revisit, content_changed,\n                                                 is_duplicate)\n\n            # Extract URLs from page (always do this for revisits to find new links)\n            found_urls = self._extract_urls_from_page(html_content, current_url)\n\n            for url in found_urls:\n\n                normalized_found_url = self._normalize_url_with_language(url)\n                # Only add if normalized URL hasn't been seen\n                if not self.storage.is_url_visited(normalized_found_url):\n                    new_urls.append((normalized_found_url, current_url, depth + 1, False))\n\n                # # For revisits, add all URLs regardless of visit status\n                # if is_revisit or not self.storage.is_url_visited(url):\n                #     new_urls.append((url, current_url, depth + 1, False))\n\n            self.telemetry.update_urls_discovered(len(new_urls))\n\n            if content_changed:\n                self.logger.info(f\"Content changed! Found {len(new_urls)} URLs on {current_url}\")\n            else:\n                self.logger.info(f\"Found {len(new_urls)} URLs on {current_url}\")\n\n            # Periodic state saving\n            with self.stats_lock:\n                self.pages_processed += 1\n                if self.pages_processed % self.save_interval == 0:\n                    self._save_state()\n                    # self.logger.info('********Sunny Saved State*******')\n\n        except Exception as e:\n            print(f\"Error processing {current_url}: {e}\")\n            self._categorize_and_log_error(e, current_url)\n            return new_urls\n\n        return new_urls\n\n    def _categorize_and_log_error(self, error: Exception, url: str):\n        \"\"\"Categorize and log errors for better debugging.\"\"\"\n        error_type = type(error).__name__\n        is_recoverable = True\n\n        # Categorize error types\n        if isinstance(error, (requests.exceptions.Timeout, requests.exceptions.ConnectionError)):\n            error_category = \"network\"\n        elif isinstance(error, requests.exceptions.HTTPError):\n            error_category = \"http\"\n            is_recoverable = error.response.status_code < 500 if hasattr(error, 'response') else True\n        elif isinstance(error, (AttributeError, TypeError)):\n            error_category = \"parsing\"\n            is_recoverable = False\n        else:\n            error_category = \"unknown\"\n\n        with self.error_lock:\n            self.error_categories[error_category] += 1\n\n        self.telemetry.update_error_count(error_category, is_recoverable)\n\n    def _fetch_page(self, url: str) -> Optional[Tuple[str, str, int, float, int, str]]:\n        \"\"\"Enhanced fetch page method with better error handling.\"\"\"\n        session = self._get_session()\n\n        for attempt in range(self.max_retries):\n            start_time = time.time()\n            response = None\n\n            try:\n                # Check if we should stop\n                if self.killer.should_stop():\n                    return None\n\n                # Add random delay to avoid overwhelming servers\n                if self.request_delay > 0:\n                    delay = self.request_delay + (attempt * 0.1)\n                    time.sleep(delay)\n\n                response = session.get(\n                    url,\n                    timeout=self.timeout,\n                    allow_redirects=True,\n                    stream=False\n                )\n\n                if response is None:\n                    self.logger.error(f\"Response is None for {url}\")\n                    continue\n\n                response_time = time.time() - start_time\n\n                # Safely get content size\n                content_size = len(response.content) if hasattr(response, 'content') and response.content else 0\n\n                if response.status_code == 200:\n                    # Safely get content type\n                    content_type = response.headers.get('content-type', '').lower() if hasattr(response,\n                                                                                               'headers') else ''\n\n                    if 'text/html' in content_type:\n                        # Check content length\n                        if content_size > 10 * 1024 * 1024:  # 10MB limit\n                            self.logger.warning(f\"Content too large, skipping: {url}\")\n                            return None\n\n                        # Safely get response text\n                        response_text = response.text if hasattr(response, 'text') and response.text else ''\n\n                        if not response_text:\n                            self.logger.warning(f\"Response text is empty for {url}\")\n                            return None\n\n                        # Detect language\n                        detected_language = self._detect_language(response_text)\n                        page_title = self._get_page_title(response_text)\n\n                        return (response_text, page_title, response.status_code,\n                                response_time, content_size, detected_language)\n                    else:\n                        if self.debug_mode:\n                            self.logger.debug(f\"Skipping non-HTML content: {url}\")\n                        return None\n                else:\n                    # self.logger.warning(f\"HTTP {response.status_code} for {url}\")\n                    if response.status_code in [404, 403, 410, 429]:\n                        return None\n\n            except requests.exceptions.Timeout:\n                self.logger.warning(f\"Timeout for {url} (attempt {attempt + 1})\")\n            except requests.exceptions.ConnectionError:\n                self.logger.warning(f\"Connection error for {url} (attempt {attempt + 1})\")\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request error for {url} (attempt {attempt + 1}): {e}\")\n            except Exception as e:\n                self.logger.error(f\"Unexpected error for {url}: {e}\")\n\n            if attempt < self.max_retries - 1:\n                time.sleep(2 ** attempt + random.uniform(0, 1))\n\n        return None\n\n    def _get_session(self):\n        \"\"\"Get thread-local session with enhanced error handling.\"\"\"\n        try:\n            with self.session_lock:\n                if not hasattr(self.session_storage, 'session') or self.session_storage.session is None:\n                    self.session_storage.session = requests.Session()\n\n                    # Add to weak set for cleanup\n                    self.session_pool.add(self.session_storage.session)\n\n                    # Set headers safely\n                    self.session_storage.session.headers.update({\n                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n                        'Accept-Language': 'en-US,en;q=0.5',\n                        'Accept-Encoding': 'gzip, deflate',\n                        'Connection': 'keep-alive',\n                        'Upgrade-Insecure-Requests': '1'\n                    })\n\n                return self.session_storage.session\n        except Exception as e:\n            self.logger.error(f\"Error creating session: {e}\")\n            # Return a new session as fallback\n            return requests.Session()\n\n    def _get_page_title(self, html: str) -> str:\n        \"\"\"Extract page title from HTML with enhanced error handling.\"\"\"\n        try:\n            if not html:\n                return \"\"\n\n            soup = BeautifulSoup(html, 'html.parser')\n            if not soup:\n                return \"\"\n\n            title_tag = soup.find('title')\n            if title_tag and hasattr(title_tag, 'get_text'):\n                title_text = title_tag.get_text()\n                if title_text:\n                    return title_text.strip()[:200]\n            return \"\"\n        except Exception as e:\n            self.logger.error(f\"Error extracting page title: {e}\")\n            return \"\"\n\n    def _extract_urls_from_page(self, html: str, base_url: str) -> List[str]:\n        \"\"\"Extract all HTTPS URLs from a page with enhanced error handling.\"\"\"\n        urls = []\n        try:\n            if not html:\n                return urls\n\n            soup = BeautifulSoup(html, 'html.parser')\n            if not soup:\n                return urls\n\n            # Extract from anchor tags\n            for element in soup.find_all('a', href=True):\n                if not element:\n                    continue\n\n                href = element.get('href')\n                if not href:\n                    continue\n\n                # Handle different types of URLs\n                absolute_url = None\n                if href.startswith('http'):\n                    absolute_url = href\n                elif href.startswith('//'):\n                    absolute_url = 'https:' + href\n                elif href.startswith('/'):\n                    absolute_url = urljoin(base_url, href)\n                elif not href.startswith(('#', 'mailto:', 'javascript:', 'tel:')):\n                    absolute_url = urljoin(base_url, href)\n                else:\n                    continue\n\n                if absolute_url:\n                    normalized_url = self._normalize_url(absolute_url)\n                    if (normalized_url and\n                            self._is_valid_https_url(normalized_url) and\n                            self._is_english_url(normalized_url)):\n                        urls.append(normalized_url)\n\n            # Also check link tags\n            for element in soup.find_all('link', href=True):\n                if not element:\n                    continue\n\n                href = element.get('href')\n                if href and href.startswith('http'):\n                    normalized_url = self._normalize_url(href)\n                    if (normalized_url and\n                            self._is_valid_https_url(normalized_url) and\n                            self._is_english_url(normalized_url)):\n                        urls.append(normalized_url)\n\n        except Exception as e:\n            self.logger.error(f\"Error extracting URLs from {base_url}: {e}\")\n\n        return list(set(urls))  # Remove duplicates\n\n    def _update_stats(self, is_new: bool):\n        \"\"\"Thread-safe statistics update.\"\"\"\n        with self.stats_lock:\n            if is_new:\n                self.new_urls_found += 1\n                self.pages_since_last_new = 0\n            else:\n                self.pages_since_last_new += 1\n\n    def _worker_thread(self):\n        \"\"\"Enhanced worker thread with better error handling and resource management.\"\"\"\n        thread_name = threading.current_thread().name\n        self.logger.info(f\"Worker thread {thread_name} started\")\n\n        try:\n            while self.running and not self.killer.should_stop():\n                try:\n                    # Get URL from queue with timeout\n                    url_data = self.urls_to_visit.get(timeout=self.queue_timeout)\n\n                    # Check for poison pill\n                    if url_data is None:\n                        break\n\n                    # Update queue size for telemetry\n                    self.telemetry.update_queue_size(self.urls_to_visit.qsize())\n\n                    # Process the URL\n                    new_urls = self._process_single_url(url_data)\n\n                    # Add new URLs to queue\n                    for new_url_data in new_urls:\n                        if not self.killer.should_stop():\n                            self.urls_to_visit.put(new_url_data)\n\n                    # Check for diminishing returns\n                    with self.stats_lock:\n                        if (self.pages_since_last_new > self.diminishing_returns_pages and\n                                self.new_urls_found < self.diminishing_returns_threshold):\n                            self.logger.info(f\"Diminishing returns detected in {thread_name}\")\n                            break\n\n                    # Memory check\n                    if self.memory_monitor.is_memory_limit_exceeded():\n                        print(f\"Memory limit exceeded in {thread_name}\")\n                        break\n\n                except Empty:\n                    # Queue is empty, check if we should continue\n                    if self.urls_to_visit.empty():\n                        self.logger.info(f\"Queue empty in {thread_name}, checking for more work...\")\n                        time.sleep(1)\n                        if self.urls_to_visit.empty():\n                            break\n                    continue\n\n                except Exception as e:\n                    self.logger.error(f\"Worker thread {thread_name} error: {e}\")\n                    with self.error_lock:\n                        self.error_categories[\"worker\"] += 1\n                    time.sleep(1)\n\n        except Exception as e:\n            self.logger.error(f\"Fatal error in worker thread {thread_name}: {e}\")\n        finally:\n            self.logger.info(f\"Worker thread {thread_name} finished\")\n\n    def _find_unvisited_discovered_urls(self, limit: int = 100) -> List[Tuple[str, str, int]]:\n        \"\"\"Find unvisited discovered URLs for incremental crawling.\"\"\"\n        try:\n            if hasattr(self.storage, 'get_unvisited_discovered_urls'):\n                return self.storage.get_unvisited_discovered_urls(limit)\n            else:\n                # Fallback for basic storage\n                return []\n        except Exception as e:\n            self.logger.error(f\"Error finding unvisited URLs: {e}\")\n            return []\n\n    def _load_state(self):\n        \"\"\"Load previous crawler state.\"\"\"\n        try:\n            state = self.storage.load_crawler_state()\n            if state:\n                self.logger.info(\"Previous crawler state loaded successfully\")\n            else:\n                self.logger.info(\"No previous state found, starting fresh\")\n        except Exception as e:\n            self.logger.error(f\"Error loading state: {e}\")\n\n    def _save_state(self):\n        \"\"\"Save current crawler state.\"\"\"\n        import datetime\n\n        try:\n            # Collect pending URLs from queue\n            pending_urls = []\n            temp_queue = Queue()\n\n            while not self.urls_to_visit.empty():\n                try:\n                    url_data = self.urls_to_visit.get_nowait()\n                    pending_urls.append(url_data)\n                    temp_queue.put(url_data)\n                except Empty:\n                    break\n\n            # Restore queue\n            while not temp_queue.empty():\n                try:\n                    self.urls_to_visit.put(temp_queue.get_nowait())\n                except Empty:\n                    break\n\n            state_data = {\n                'pending_urls': pending_urls,\n                'pages_processed': self.pages_processed,\n                'new_urls_found': self.new_urls_found,\n                'error_categories': dict(self.error_categories),\n                'timestamp': datetime.datetime.now().isoformat()\n            }\n\n            self.storage.save_crawler_state(state_data)\n            self.logger.info(\"Crawler state saved\")\n\n        except Exception as e:\n            print(f\"Error saving state: {e}\")\n            # sys.exit(0)\n\n    def stop(self):\n        \"\"\"Stop the crawler gracefully.\"\"\"\n        self.logger.info(\"Stopping crawler gracefully...\")\n        self.killer.stop()\n        self.running = False\n\n    def crawl(self):\n        \"\"\"Enhanced crawl method with comprehensive content storage.\"\"\"\n        storage_type = \"Snowflake database\"\n        self.logger.info(f\"Starting enhanced incremental multi-threaded crawl with {self.max_workers} workers...\")\n        self.logger.info(f\"Storage mode: {storage_type}\")\n        self.logger.info(\n            f\"Content storage: {'enabled' if self.store_raw_html or self.store_cleaned_text else 'disabled'}\")\n\n        self.logger.info(\n            f\"Content change detection: {'enabled' if self.enable_content_change_detection else 'disabled'}\")\n        self.logger.info(f\"Revisit interval: {self.revisit_interval_hours} hours\")\n        self.logger.info(\n            f\"URL language filtering: {'enabled' if self.enable_url_language_filtering else 'disabled'}\")\n        self.logger.info(\n            f\"Content language filtering: {'enabled' if self.enable_language_filtering else 'disabled'}\")\n        self.logger.info(\"Press Ctrl+C to stop gracefully\")\n\n        # Test connection\n        # if not test_snowflake_connection():\n        #     self.logger.error(\"Failed to connect to Snowflake. Aborting.\")\n        #     return\n\n        self.start_time = time.time()\n        self._last_save = self.start_time\n        initial_count = self.storage.get_discovered_urls_count()\n\n        # Log incremental mode status\n        initial_discovered = self.storage.get_discovered_urls_count()\n        initial_visited = self.storage.get_visited_urls_count()\n\n        if initial_discovered > 0:\n            self.logger.info(f\"INCREMENTAL MODE: Resuming with {initial_discovered} discovered URLs\")\n            self.logger.info(f\"INCREMENTAL MODE: {initial_visited} URLs already visited\")\n            self.logger.info(f\"INCREMENTAL MODE: {initial_discovered - initial_visited} URLs pending\")\n        else:\n            self.logger.info(\"FRESH START: No previous data found\")\n\n        # Start telemetry\n        self.telemetry.start_telemetry()\n\n        # Create and start worker threads\n        threads = []\n        for i in range(self.max_workers):\n            thread = threading.Thread(target=self._worker_thread, name=f\"Worker-{i + 1}\")\n            thread.daemon = True\n            thread.start()\n            threads.append(thread)\n\n        # Update active thread count\n        self.telemetry.update_active_threads(len(threads))\n\n        try:\n            while self.running and not self.killer.should_stop():\n                # Periodic state saves during main loop\n                current_time = time.time()\n                if current_time - self._last_save > 600:  # Save state every minute\n                    self._save_state()\n                    self._last_save = current_time\n\n                # Check if queue is empty and no threads are working\n                if self.urls_to_visit.empty():\n                    # Wait a bit to see if new URLs are added\n                    time.sleep(2)\n                    if self.urls_to_visit.empty():\n                        self.logger.info(\"No more URLs to process. Stopping crawl.\")\n                        break\n                else:\n                    time.sleep(1)\n\n        except KeyboardInterrupt:\n            self.logger.info(\"Crawl interrupted by user (Ctrl+C)\")\n            self.killer.stop()\n        finally:\n            # Signal threads to stop\n            self.running = False\n\n            # Add poison pills to wake up threads\n            for _ in range(self.max_workers):\n                try:\n                    self.urls_to_visit.put(None, timeout=1)\n                except:\n                    pass\n\n            # Wait for threads to finish\n            for thread in threads:\n                thread.join(timeout=5)\n\n            # Stop telemetry and show final report\n            self.telemetry.stop_telemetry()\n            print(\"\\n\" + \"=\" * 80)\n            print(\"FINAL TELEMETRY REPORT\")\n            print(\"=\" * 80)\n            print(self.telemetry.get_final_report())\n            print(\"=\" * 80)\n\n            # Final save\n            self._save_state()\n\n            # Export to CSV if requested\n            print('********************Loading Tables in Snowflake*************')\n            if self.export_csv:\n                # self.storage.export_to_csv(csv_filename)\n                upload_csv_to_snowflake({},\n                                        self.table_prefix\n                                        )\n\n            # Close storage if needed\n            if hasattr(self.storage, 'close'):\n                self.storage.close()\n\n            # Cleanup sessions\n            self._cleanup_sessions()\n\n            # Show enhanced final statistics\n            self._show_final_statistics(initial_count, storage_type)\n\n    def _cleanup_sessions(self):\n        \"\"\"Clean up HTTP sessions.\"\"\"\n        try:\n            for session in self.session_pool:\n                if session:\n                    session.close()\n            self.logger.info(\"HTTP sessions cleaned up\")\n        except Exception as e:\n            self.logger.error(f\"Error cleaning up sessions: {e}\")\n\n    def _show_final_statistics(self, initial_count: int, storage_type: str):\n        \"\"\"Show comprehensive final statistics.\"\"\"\n        final_count = self.storage.get_discovered_urls_count()\n        new_urls_this_run = final_count - initial_count\n\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"FINAL STATISTICS ({storage_type})\")\n        print(\"=\" * 60)\n        print(f\"Total discovered URLs: {final_count:,}\")\n        print(f\"New URLs this run: {new_urls_this_run:,}\")\n\n        # Show statistics\n        stats = self.storage.get_statistics()\n        if stats:\n            print(f\"Total visited URLs: {stats.get('total_visited', 0):,}\")\n\n            lang_dist = stats.get('language_distribution', {})\n            if lang_dist:\n                print(\"\\nLanguage distribution:\")\n                for lang, count in list(lang_dist.items())[:10]:\n                    print(f\"  {lang}: {count:,}\")\n\n            depth_dist = stats.get('depth_distribution', {})\n            if depth_dist:\n                print(\"\\nDepth distribution:\")\n                for depth, count in depth_dist.items():\n                    print(f\"  Depth {depth}: {count:,}\")\n\n        # Show error statistics\n        if self.error_categories:\n            print(\"\\nError categories:\")\n            for category, count in self.error_categories.items():\n                print(f\"  {category}: {count:,}\")\n\n        print(\"=\" * 60)\n\n\ndef main():\n    \"\"\"Enhanced main function with database-only configuration.\"\"\"\n\n    print(\"🕷️  Enhanced Incremental Web Crawler - Database Mode\")\n    print(\"=\" * 75)\n    print(\"Starting with predefined configuration...\")\n    print(\"=\" * 75)\n\n    # ===== ENHANCED CONFIGURATION SECTION =====\n    # Modify these settings directly in the code\n    # private_key_path = get_private_key()\n    # Snowflake Configuration (required for database mode)\n    # SNOWFLAKE_CONFIG = {\n    #     'user': os.environ['SNOWFLAKE_USER'],\n    #     'host': os.environ['HOST'],\n    #     'port': '443',\n    #     'account': os.environ['SNOWFLAKE_ACCOUNT'],\n    #     'authenticator': 'externalbrowser',\n    #     'warehouse': os.environ['SNOWFLAKE_WAREHOUSE'],  # Optional\n    #     'database': os.environ['SNOWFLAKE_DATABASE'],\n    #     'schema': os.environ['SNOWFLAKE_SCHEMA'],\n    #     'session_parameters': {'ABORT_DETACHED_QUERY': 'TRUE'}\n    # }\n\n    SNOWFLAKE_CONFIG = {}\n\n    # Crawling Targets\n    STARTING_URLS = [\n        'https://docs.snowflake.com/en/',\n        'https://quickstarts.snowflake.com/en/',\n        'https://docs.snowflake.com/en/sitemap.xml',\n        # Add more starting URLs here\n    ]\n\n    ALLOWED_DOMAINS = [\n        'snowflake.com',\n        # 'quickstarts.snowflake.com',\n        # Add more allowed domains here\n    ]\n\n    # Crawling Parameters\n    MAX_DEPTH = 12\n    MAX_WORKERS = 40\n    REQUEST_DELAY = 1  # seconds between requests\n\n    # Enhanced Content Storage Configuration\n    STORE_RAW_HTML = False  # Store full HTML content\n    STORE_CLEANED_TEXT = False  # Store cleaned text content\n    STORE_EXTRACTED_DATA = False  # Store structured data\n    MAX_CONTENT_SIZE = 5000000  # 5MB limit per page\n    EXTRACT_METADATA = False  # Extract meta tags\n    EXTRACT_HEADINGS = False  # Extract heading structure\n    EXTRACT_IMAGES = False  # Extract image information\n    EXTRACT_LINKS = True  # Extract all links\n    EXTRACT_STRUCTURED_DATA = False  # Extract JSON-LD and microdata\n\n    # Incremental Features\n    ENABLE_CONTENT_CHANGE_DETECTION = True\n    REVISIT_INTERVAL_HOURS = 24*7  # Check for new URLs every 7 Days\n    ENABLE_LANGUAGE_FILTERING = True\n    ENABLE_URL_LANGUAGE_FILTERING = True\n\n    # Max runtime as failsafe (Can be configured for one-time vs incremental)\n    MAX_RUNTIME_HOURS = 4\n\n    # File Configuration\n    TABLE_PREFIX = 'CRAWLER'\n\n    # Debug Mode\n    DEBUG_MODE = False\n\n    USE_DATABASE = False\n\n    # ===== END CONFIGURATION SECTION =====\n\n    # Build enhanced configuration dictionary\n    config = {\n        # Snowflake configuration\n        'snowflake_config': SNOWFLAKE_CONFIG,\n        'table_prefix': TABLE_PREFIX,\n        'max_cache_size': 10000,\n\n        # Storage configuration\n        'use_database': USE_DATABASE,  # Set to False for CSV storage\n\n        # Crawling targets\n        'starting_urls': STARTING_URLS,\n        'allowed_domains': ALLOWED_DOMAINS,\n\n        # Crawling parameters\n        'max_depth': MAX_DEPTH,\n        'max_workers': MAX_WORKERS,\n        'request_delay': REQUEST_DELAY,\n        'timeout': 15,\n        'max_retries': 3,\n        'queue_timeout': 10,\n\n        # Time-based kill switch\n        'max_runtime_hours': MAX_RUNTIME_HOURS,\n\n        # Enhanced content storage options\n        'store_raw_html': STORE_RAW_HTML,\n        'store_cleaned_text': STORE_CLEANED_TEXT,\n        'store_extracted_data': STORE_EXTRACTED_DATA,\n        'max_content_size': MAX_CONTENT_SIZE,\n        'extract_metadata': EXTRACT_METADATA,\n        'extract_headings': EXTRACT_HEADINGS,\n        'extract_images': EXTRACT_IMAGES,\n        'extract_links': EXTRACT_LINKS,\n        'extract_structured_data': EXTRACT_STRUCTURED_DATA,\n\n        # Incremental crawling features\n        'enable_content_change_detection': ENABLE_CONTENT_CHANGE_DETECTION,\n        'revisit_interval_hours': REVISIT_INTERVAL_HOURS,\n        'max_revisit_urls_per_run': 15000,\n        'content_change_threshold': 0.1,\n        'force_revisit_depth': 1,\n\n        # Language filtering\n        'enable_language_filtering': ENABLE_LANGUAGE_FILTERING,\n        'enable_url_language_filtering': ENABLE_URL_LANGUAGE_FILTERING,\n\n        # Performance and monitoring\n        'save_interval': 100,\n        'telemetry_interval': 360,\n        'diminishing_returns_threshold': 10,\n        'diminishing_returns_pages': 50,\n\n        # Output options\n        'export_csv': True,\n        'debug_mode': DEBUG_MODE,\n    }\n\n    # Enhanced configuration validation\n    validation_errors = []\n\n    if not config['starting_urls']:\n        validation_errors.append(\"No starting URLs specified\")\n\n    if not config['allowed_domains']:\n        validation_errors.append(\"No allowed domains specified\")\n\n    # Check for valid URLs\n    for url in config['starting_urls']:\n        if not (url.startswith('http://') or url.startswith('https://')):\n            validation_errors.append(f\"Invalid URL format: {url}\")\n\n    # Validate content storage configuration\n    if config['store_raw_html'] and config['max_content_size'] < 10000:\n        validation_errors.append(\"max_content_size too small for raw HTML storage\")\n\n    # if not config['snowflake_config']:\n    #     validation_errors.append(\"Snowflake configuration required for database mode\")\n\n    if validation_errors:\n        print(\"\\n❌ Configuration Validation Errors:\")\n        for error in validation_errors:\n            print(f\"  • {error}\")\n        print(\"\\nPlease fix the configuration in the main() function.\")\n        return\n\n    # Display enhanced configuration summary\n    print(\"\\n\" + \"=\" * 70)\n    print(\"🚀 ENHANCED CRAWLER CONFIGURATION SUMMARY\")\n    print(\"=\" * 70)\n    print(\n        f\"📁 Content Storage: {'Enabled' if config['store_raw_html'] or config['store_cleaned_text'] else 'Disabled'}\")\n    print(f\"🔍 Raw HTML Storage: {'Enabled' if config['store_raw_html'] else 'Disabled'}\")\n    print(f\"📝 Cleaned Text Storage: {'Enabled' if config['store_cleaned_text'] else 'Disabled'}\")\n    print(f\"🏷️  Metadata Extraction: {'Enabled' if config['extract_metadata'] else 'Disabled'}\")\n    print(f\"📊 Structured Data: {'Enabled' if config['extract_structured_data'] else 'Disabled'}\")\n    print(f\"🖼️  Image Extraction: {'Enabled' if config['extract_images'] else 'Disabled'}\")\n    print(f\"🔗 Link Extraction: {'Enabled' if config['extract_links'] else 'Disabled'}\")\n    print(f\"📏 Max Content Size: {config['max_content_size']:,} bytes\")\n    print(f\"🔄 Content Change Detection: {'Enabled' if config['enable_content_change_detection'] else 'Disabled'}\")\n    print(f\"⏰ Revisit Interval: {config['revisit_interval_hours']} hours\")\n    print(f\"🌐 Starting URLs: {len(config['starting_urls'])}\")\n    print(f\"🏠 Allowed Domains: {len(config['allowed_domains'])}\")\n    print(f\"🕳️  Max Depth: {config['max_depth']}\")\n    print(f\"👥 Workers: {config['max_workers']}\")\n    print(f\"⏱️  Request Delay: {config['request_delay']}s\")\n    print(f\"🐛 Debug Mode: {'Enabled' if config['debug_mode'] else 'Disabled'}\")\n\n    print(f\"❄️  Snowflake Account: {config['snowflake_config'].get('account', 'Not specified')}\")\n    print(f\"🗄️  Database: {config['snowflake_config'].get('database', 'Not specified')}\")\n    print(f\"📋 Schema: {config['snowflake_config'].get('schema', 'Not specified')}\")\n    print(f\"🏢 Warehouse: {config['snowflake_config'].get('warehouse', 'Default')}\")\n\n    print(\"=\" * 70)\n\n    # Initialize and run enhanced crawler\n    crawler = None\n    try:\n        print(\"\\n🔄 Initializing enhanced crawler with content storage...\")\n        crawler = ThreadSafeCrawler(config)\n\n        print(\"🕷️  Starting enhanced crawl with content extraction...\")\n        print(\"📊 Content will be stored with the following features:\")\n        if config['store_raw_html']:\n            print(\"  • Raw HTML content\")\n        if config['store_cleaned_text']:\n            print(\"  • Cleaned text content\")\n        if config['extract_metadata']:\n            print(\"  • Meta tags (description, keywords)\")\n        if config['extract_headings']:\n            print(\"  • Heading structure (H1-H6)\")\n        if config['extract_images']:\n            print(\"  • Image information and metadata\")\n        if config['extract_links']:\n            print(\"  • All extracted links\")\n        if config['extract_structured_data']:\n            print(\"  • Structured data (JSON-LD, microdata)\")\n\n        crawler.crawl()\n\n    except KeyboardInterrupt:\n        print(\"\\n\\n⚠️  Received Ctrl+C, stopping crawler gracefully...\")\n        if crawler:\n            crawler.stop()\n\n        print(\"✅ Enhanced crawler stopped gracefully.\")\n\n    except Exception as e:\n        print(f\"\\n❌ Unexpected error occurred: {e}\")\n        logging.error(f\"Fatal error in main: {e}\")\n        import traceback\n        if config.get('debug_mode'):\n            traceback.print_exc()\n        else:\n            print(\"Set DEBUG_MODE=True for detailed error information.\")\n\n    finally:\n        if crawler:\n            print(\"\\n🧹 Cleaning up enhanced crawler resources...\")\n            try:\n                if hasattr(crawler, 'storage') and hasattr(crawler.storage, 'close'):\n                    crawler.storage.close()\n            except Exception as e:\n                logging.error(f\"Error during cleanup: {e}\")\n\n            if config.get('export_csv'):\n                # self.storage.export_to_csv(csv_filename)\n                upload_csv_to_snowflake({}, 'crawler')\n\n        print(\"\\n✅ Enhanced crawler session completed.\")\n        print(\"📊 Check your Snowflake database for enhanced content results.\")\n\n        if config['store_raw_html'] or config['store_cleaned_text']:\n            print(\"🎉 Content storage features were enabled - check your database for rich content data!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7c4d41e2-33e8-48bd-8f09-c77397b5b11d",
   "metadata": {
    "language": "python",
    "name": "execute_crawler",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "if __name__ == \"__main__\":\n    main()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec2d8886-d8e3-4aa8-ac9c-4d41d83f1186",
   "metadata": {
    "language": "python",
    "name": "check_files",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# import os\n# # import pandas as pd\n\n# os.listdir()\n\n# # # if os.path.exists('crawler_discovered_urls.csv'):\n# # #     print('here')\n# # #     df = pd.read_csv('crawler_discovered_urls.csv')\n\n\n# # # df[df['content_size'] == 'text/html']",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee07e7ed-1ef5-49ea-9c5c-61ecb18dcced",
   "metadata": {
    "language": "python",
    "name": "read_csv_and_validate",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# df = pd.read_csv('crawler_discovered_urls.csv');\n\n# # df.columns = df.columns.str.upper()\n\n# # from snowflake.snowpark.context import get_active_session\n# # session = get_active_session()\n\n# # # Convert pandas DataFrame directly to Snowpark DataFrame\n# # snow_df = session.create_dataframe(df)\n\n# df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "523e0e28-5efc-47a7-b1b9-97f54b48c1f1",
   "metadata": {
    "name": "export_csv",
    "collapsed": false
   },
   "source": "***If \"export_csv\" is set to False, Upload CSV files to Snowflake using below method***"
  },
  {
   "cell_type": "code",
   "id": "95d080e6-f728-463d-b081-76142b2aca97",
   "metadata": {
    "language": "python",
    "name": "load_data_to_snowflake",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Upload files to snowflake tables, keep the table prefix as lower case (filenames are case sensitive)\n# upload_csv_to_snowflake({},'crawler')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51276f39-d114-49ec-a367-bb6e19985866",
   "metadata": {
    "language": "python",
    "name": "get_page_content",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "def post_process_missing_content(batch_size: int = 50, max_retries: int = 3):\n    \"\"\"\n    Post-process URLs with missing content in the discovered_urls table.\n    \n    Args:\n        batch_size: Number of URLs to process in each batch\n        max_retries: Maximum number of retry attempts for failed requests\n    \"\"\"\n    from snowflake.snowpark.context import get_active_session\n    import requests\n    from bs4 import BeautifulSoup\n    import time\n    import json\n    from snowflake.snowpark.functions import lit\n    \n    session = get_active_session()\n    \n    # Find URLs with missing content\n    missing_content_query = \"\"\"\n    SELECT URL \n    FROM CRAWLER_DISCOVERED_URLS \n    WHERE CLEANED_TEXT = '' OR CLEANED_TEXT IS NULL \n        AND STATUS_CODE = 200 \n        AND URL ILIKE 'https://%'\n    ORDER BY LAST_VISITED DESC\n    \"\"\"\n    \n    urls_to_process = session.sql(missing_content_query).collect()\n    total_urls = len(urls_to_process)\n    \n    if total_urls == 0:\n        print(\"No URLs with missing content found.\")\n        return\n    \n    print(f\"Found {total_urls} URLs with missing content\")\n    \n    # Initialize request session\n    req_session = requests.Session()\n    req_session.headers.update({\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',\n        'Accept-Language': 'en-US,en;q=0.5'\n    })\n    \n    # Process URLs in batches\n    for i in range(0, total_urls, batch_size):\n        batch = urls_to_process[i:i + batch_size]\n        content_data = []\n        \n        print(f\"\\nProcessing batch {i//batch_size + 1} of {(total_urls + batch_size - 1)//batch_size}\")\n        \n        for row in batch:\n            url = row['URL']\n            retries = 0\n            \n            while retries < max_retries:\n                try:\n                    response = req_session.get(url, timeout=10)\n                    if response.status_code == 200:\n                        soup = BeautifulSoup(response.text, 'html.parser')\n                        \n                        # Extract content\n                        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n                            tag.decompose()\n                            \n                        # Get cleaned text\n                        cleaned_text = ' '.join(soup.stripped_strings)\n                        cleaned_text = cleaned_text[:50000]\n\n                        # print('cleaned_text', cleaned_text)\n                        \n                        # Extract meta description\n                        meta_desc = soup.find('meta', attrs={'name': 'description'})\n                        meta_description = meta_desc.get('content', '')[:500] if meta_desc else \"\"\n                        \n                        # Extract headings\n                        headings = {}\n                        for i in range(1, 7):\n                            heading_tags = soup.find_all(f'h{i}')\n                            if heading_tags:\n                                headings[f'h{i}'] = [tag.get_text().strip()[:200] for tag in heading_tags[:10]]\n                        \n                        content_data.append({\n                            'URL': url,\n                            'CLEANED_TEXT': cleaned_text,\n                            'META_DESCRIPTION': meta_description,\n                            'HEADINGS': json.dumps(headings)\n                        })\n                        \n                        # print(f\"✓ Processed {url}\")\n                        break\n                    else:\n                        print(f\"✗ Failed to fetch {url}: HTTP {response.status_code}\")\n                        break\n                        \n                except Exception as e:\n                    retries += 1\n                    if retries == max_retries:\n                        print(f\"✗ Failed to process {url} after {max_retries} attempts: {str(e)}\")\n                    time.sleep(1)\n            \n            time.sleep(0.5)\n        \n        # Bulk update the database\n        if content_data:\n            try:\n                # Create a DataFrame from the content data\n                update_df = session.create_dataframe(content_data)\n                update_df.write.mode(\"overwrite\").save_as_table(\"TEMP_UPDATES_TABLE\")\n            \n                \n                # Perform the merge using Snowpark DataFrame operations\n                merge_query = f\"\"\"\n                MERGE INTO CRAWLER_DISCOVERED_URLS target\n                USING (SELECT * FROM TEMP_UPDATES_TABLE) source\n                ON target.URL = source.URL\n                WHEN MATCHED THEN UPDATE SET\n                    CLEANED_TEXT = source.CLEANED_TEXT,\n                    META_DESCRIPTION = source.META_DESCRIPTION,\n                    HEADINGS = PARSE_JSON(source.HEADINGS),\n                    UPDATED_AT = CURRENT_TIMESTAMP()\n                \"\"\"\n                \n                session.sql(merge_query).collect()\n                print(f\"\\n✓ Successfully updated {len(content_data)} URLs in batch\")\n            except Exception as e:\n                print(f\"\\n✗ Failed to update batch: {str(e)}\")\n        \n        print(f\"\\nProgress: {min(i + batch_size, total_urls)}/{total_urls} URLs processed\")\n\n    print(\"\\nContent post-processing completed!\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29120e6f-94ff-4823-906b-9263a638ff8d",
   "metadata": {
    "language": "python",
    "name": "execute_get_content",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "post_process_missing_content(100, 3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03929a1c-6cf0-4644-b1ba-60a8f4ebf61a",
   "metadata": {
    "language": "sql",
    "name": "create_cortex_search_service"
   },
   "outputs": [],
   "source": "-- Create a Cortex Search Service once the URL are scraped and content is pulled successfully\nCREATE OR REPLACE CORTEX SEARCH SERVICE DOC_SEARCHER\n  ON chunked_data\n  WAREHOUSE = <wh_name>\n  TARGET_LAG = '7 days'  -- Run every 7 Days\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n  INITIALIZE = ON_CREATE\n  AS \n    SELECT u.*, index, value::string as chunked_data FROM <fully_qualified_table_name> u,\n    LATERAL FLATTEN(input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (cleaned_text,'markdown',2000,300))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d104460-b13b-45a3-9ca3-0aa90648cf40",
   "metadata": {
    "name": "check_cortex_search_service",
    "collapsed": true
   },
   "source": "-- -- Cortex Search Preview query\n-- SELECT\n--   SNOWFLAKE.CORTEX.SEARCH_PREVIEW (\n--       '<fully_qualified_service_name>',\n--       '{\n--           \"query\": \"what connectors are available in openflow\",\n--           \"columns\": [\"CHUNKED_DATA\", \"URL\"],\n--           \"limit\": 20\n--       }'\n--   );"
  }
 ]
}